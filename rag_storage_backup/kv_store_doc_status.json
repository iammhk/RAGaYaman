{
  "doc-e8ddfee49a0b9773c089a5570bc23814": {
    "status": "failed",
    "error": "index 0 is out of bounds for axis 0 with size 0",
    "content": "From Understanding to Utilization: A Survey on Explainability for Large\nLanguage Models\nHaoyan Luo\nImperial College London\nh.luo23@imperial.ac.ukLucia Specia\nImperial College London\nl.specia@imperial.ac.uk\nAbstract\nExplainability for Large Language Models\n(LLMs) is a critical yet challenging aspect of\nnatural language processing. As LLMs are\nincreasingly integral to diverse applications,\ntheir “black-box” nature sparks significant con-\ncerns regarding transparency and ethical use.\nThis survey underscores the imperative for in-\ncreased explainability in LLMs, delving into\nboth the research on explainability and the\nvarious methodologies and tasks that utilize\nan understanding of these models. Our fo-\ncus is primarily on pre-trained Transformer-\nbased LLMs, such as LLaMA (Touvron et al.,\n2023), which pose distinctive interpretability\nchallenges due to their scale and complexity.\nIn terms of existing methods, we classify them\ninto local and global analyses, based on their\nexplanatory objectives. When considering the\nutilization of explainability, we explore several\ncompelling methods that concentrate on model\nediting, control generation, and model enhance-\nment. Additionally, we examine representa-\ntive evaluation metrics and datasets, elucidat-\ning their advantages and limitations. Our goal\nis to reconcile theoretical and empirical under-\nstanding with practical implementation, propos-\ning exciting avenues for explanatory techniques\nand their applications in the LLMs era.\n1 Introduction\nIn the rapidly evolving field of natural language\nprocessing, Large Language Models (LLMs) have\nemerged as a cornerstone, demonstrating remark-\nable proficiency across a spectrum of tasks. De-\nspite their effectiveness, LLMs, often characterized\nas “black-box” systems, present a substantial chal-\nlenge in terms of explainability and transparency.\nThis opacity can lead to unintended consequences,\nsuch as the generation of harmful or misleading\ncontent (Gehman et al., 2020), and the occurrence\nof model hallucinations (Weidinger et al., 2021).\nThese issues underscore the urgency for improvedexplainability, not just for understanding, but for\nresponsible and ethical application.\nExplainability in LLMs serves two critical func-\ntions. For end users, it fosters trust by clarifying\nthe model’s reasoning in a nontechnical manner,\nenhancing understanding of their capabilities and\npotential flaws (Zhao et al., 2023). For develop-\ners and researchers, it offers insights into unin-\ntended biases and areas of improvement, serving\nas a tool for improving the performance of the\nmodel in downstream tasks (Bastings et al., 2022;\nMeng et al., 2023a; Li et al., 2023b). However, the\nscale of LLMs poses unique challenges to explain-\nability. Larger models with more parameters and\nextensive training data are harder to interpret. Tra-\nditional explanation methods such as SHAP values\n(Lundberg and Lee, 2017) become less practical\nfor these large-scale models (Zhao et al., 2023).\nMoreover, a comprehensive understanding of LLM-\nspecific phenomena, including in-context learning\n(Halawi et al., 2023; Hendel et al., 2023; Todd et al.,\n2023; Wang et al., 2023), along with addressing\nissues such as model hallucinations (Ji et al., 2023;\nChuang et al., 2023) and inherent biases (dev, 2023;\nAn and Rudinger, 2023; Schick et al., 2021), is vital\nfor ongoing refinement in model design.\nIn this survey, we focus on explainability meth-\nods for pre-trained Transformer-based LLMs, often\ntermed as base models . These models often scale\nup in training data and have billions of parameters;\nexamples include GPT-2 (Radford et al., 2019),\nGPT-J (Chen et al., 2021), GPT-3 (Brown et al.,\n2020), OPT (Yordanov et al., 2022), and LLaMA\nfamily (Touvron et al., 2023). In Section 2, we\ncategorize and pose research questions based on\nour survey. Based on this categorization, we review\nexplainability methods in Section 3, followed by a\ndiscussion in Section 4 on how these insights are\nleveraged. We further discuss the evaluation meth-\nods and metrics in Section 5. Our goal is to syn-\nthesize and critically assess contemporary research,arXiv:2401.12874v2  [cs.CL]  22 Feb 2024\naiming to bridge the gap between understanding\nand practical application of insights derived from\ncomplex language models.\n2 Overview\nThe field of LLMs is rapidly advancing, making\nexplainability not only a tool for understanding\nthese complex systems but also essential for their\nimprovement. This section categorizes current ex-\nplainability approaches, highlights the challenges\nin ethical and controllable generation, and proposes\nresearch questions for future exploration.\nCategorization of Methods We present a struc-\ntured categorization for the explainability methods\nand their applications in Figure 1. Figure 1 presents\na structured categorization of explainability meth-\nods for pre-trained language models (LMs). We di-\nvide these into two broad domains: Local Analysis\nand Global Analysis. Local Analysis covers feature\nattribution and transformer block analysis, delving\ninto detailed operations of models. Global Anal-\nysis, on the other hand, includes probing-based\nmethods and mechanistic interpretability, offering\na comprehensive understanding of model behav-\niors and capacities. Beyond understanding, we also\nexplore applications of these insights in enhanc-\ning LLM capabilities, focusing on model editing,\ncapability enhancement, and controlled generation.\n3 Explainability for Large Language\nModels\n3.1 Local Analysis\nLocal explanations in LLMs aim to elucidate how\nmodels generate specific predictions, such as senti-\nment classification or token predictions, for a given\ninput. This section categorizes local explanation\nmethods into two types: feature attribution analysis\nand analysis into individual Transformer (Vaswani\net al., 2017) components.\n3.1.1 Feature Attribution Explanation\nFeature attribution, a local method for explaining a\nprediction, analysis quantifies the relevance of each\ninput token to a model’s prediction. Given an input\ntextxwithntokens {x1, x2, ..., x n}, a pre-trained\nlanguage model foutputs f(x). Attribution meth-\nods assign a relevance score R(xi)(Modarressi\net al., 2022; Ferrando et al., 2022; Modarressi et al.,\n2023) to each token xi, reflecting its contribution\ntof(x). This category includes perturbation-based,\ngradient-based, and vector-based methods.Perturbation-Based Methods. Perturbation-\nbased methods, such as LIME (Ribeiro et al.,\n2016) and SHAP (Lundberg and Lee, 2017), alter\ninput features to observe changes in model output.\nHowever, this removal strategy assumes input\nfeatures are independent and ignores correlations\namong them. Additionally, models can be\nover-confidence even when the predictions are\nnonsensical or wrong (Feng et al., 2018). They\nalso face challenges in efficiency and reliability\nhighlighted in (Atanasova et al., 2020), leading\nto their diminished emphasis in recent attribution\nresearch.\nGradient-Based Methods. One might consider\ngradient-based explanation methods as a natural ap-\nproach for feature attribution. This type of method\ncomputes per-token importance scores (Kinder-\nmans et al., 2016) using backward gradient vec-\ntors. Techniques such as gradient ×input (Kin-\ndermans et al., 2017) and integrated gradients (IG)\n(Sundararajan et al., 2017) accumulate the gradi-\nents obtained as the input is interpolated between\na reference point and the actual input. Despite\ntheir widespread use, one main challenge of IG is\nthe computational overhead to achieve high-quality\nintegrals (Sikdar et al., 2021; Enguehard, 2023)\nTheir attribution score has also shown to be unreli-\nable in terms of faithfulness (Ferrando et al., 2022)\nand their ability to elucidate the forward dynamics\nwithin hidden states remains constrained.\nVector-Based Methods. Vector-based analyses,\nwhich focus on token representation formation,\nhave emerged as a key approach. Approaches range\nfrom global attribution from the final output layer\nto more granular, layer-wise decomposition of to-\nken representations (Chen et al., 2020; Modarressi\net al., 2022) Consider decomposing the ithtoken\nrepresentation in layer l∈ {0,1,2, ..., L, L + 1}1,\ni.e.,xl\ni∈ {xl\n1, xl\n2, ..., xl\nN}, into elemental vectors\nattributable to each of the Ninput tokens:\nxl\ni=NX\nk=1xl\ni⇐k (1)\nThe norm (Modarressi et al., 2022) or the L1 norm\n(Ferrando et al., 2022) of the attribution vector for\nthekthinput ( xl\ni⇐k) can be used to quantify its\ntotal attribution to xl\ni.\n1l= 0is the input embedding layer and l=L+ 1is the\nlanguage model head over the last decoder layer.\nModel \nEditinig\nEnhancing \nModel \nPerformance\nControllable \nGeneration\nFeature \nAttribution \nAnalysis\nExplainability \nfor \nLLMs\nProbing \nRepresentations\nCircuit \nDiscovery\nProbing-Based \nMethods\nAnalyzing \nMHSA \nSublayers\nProbing \nKnowledge\nImproving \nIn-Context \nLearning\nImproving \nUtilization \nof \nLong \nText\nGradient-Based \nMethods\nPurturbation-Based \nMethods\nLocate-Then-Edit\nHypernetwork \nKnowledge \nEditors\nPre-trained \nLLMs\nDissecting \nTransformer \nBlocks\nMechanistic \nInterpretability\nGradient-Based \nMethods\nAnalyzing \nMLP \nSublayers\nEthical \nAlignment\nReducing \nHallucination\nCausal \nTracing\nVocabulary \nLens\nLeveraging \nExplainability\nLocal \nAnalysis\nGlobal \nAnalysisFigure 1: Categorization of literature on explainability in LLMs, focusing on techniques (left, Section 3) and their applications\n(right, Section 4).\nAlthough several established strategies, such as\nattention rollouts (Abnar and Zuidema, 2020; Fer-\nrando et al., 2022; Modarressi et al., 2022), focus\non the global impact of inputs on outputs by aggre-\ngating the local behaviors of all layers, they often\nneglect Feed-Forward Network (FFN) in the analy-\nses due to its nonlinearities. Recent works address\nthis by approximating and decomposing activation\nfunctions and constructing decomposed token rep-\nresentations throughout layers (Yang et al., 2023;\nModarressi et al., 2023). Empirical evaluations\ndemonstrate the efficacy of vector-based analysis\nand exemplify the potential of such methods in\ndissecting each hidden state representation within\ntransformers.\n3.1.2 Dissecting Transformer Blocks\nTracking Transformer block’s component-by-\ncomponent internal processing can provide rich\ninformation on its intermediate processing, given\nthe stacked architecture of decoder-based language\nmodels (Kobayashi et al., 2023). In a transformer\ninference pass, the input embeddings are trans-\nformed through a sequence of Ltransformer lay-ers, each composed of a multi-head self-attention\n(MHSA) sublayer followed by an MLP sublayer\n(Vaswani et al., 2017). Formally, the representation\nxl\niof token iat layer lis obtained by:\nxl\ni=xl−1\ni+al\ni+ml\ni (2)\nwhere al\niandml\niare the outputs from the l-th\nMHSA and MLP sublayers, respectively2. While\nstudies have frequently analyzed individual Trans-\nformer components (Kobayashi et al., 2020; Modar-\nressi et al., 2022), the interaction between these\nsublayers is less explored, presenting an avenue for\nfuture research.\nAnalyzing MHSA Sublayers. Attention mecha-\nnisms in MHSA sublayers are instrumental in cap-\nturing meaningful correlations between interme-\ndiate states of input that can explain the model’s\npredictions. Visualizing attention weights and uti-\nlizing gradient attribution scores are two primary\nmethods for analyzing these sublayers (Zhao et al.,\n2For brevity, bias terms and layer normalization (Ba et al.,\n2016) are omitted, as they are nonessential for most of analy-\nsis.\n(a)\n (b)\nFigure 2: Studied role of each Transformer component. (a) gives an overview of attention mechanism in Trans-\nformers. Sizes of the colored circles illustrate the value of the scalar or the norm of the corresponding vector\n(Kobayashi et al., 2020). (b) analyzes the FFN updates in the vocabulary space, showing that each update can be\ndecomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often\nhuman-interpretable (Geva et al., 2022).\n2023). Many studies have analyzed the linguistic\ncapabilities of Transformers by tracking attention\nweights. (Abnar and Zuidema, 2020; Katz and Be-\nlinkov, 2023; Kobayashi et al., 2023). For instance,\nattention mechanisms typically prioritize specific\ntokens while diminishing the emphasis on frequent\nwords or special tokens, a phenomenon observable\nthrough norm-based analysis metrics, as illustrated\nin Figure 2(a) (Kobayashi et al., 2020). In the gra-\ndient analysis, some methods calculate gradients as\npartial derivatives of model outputs with respect to\nattention weights (Barkan et al., 2021), while others\nuse integrated gradients, which are cumulative ver-\nsions of these partial derivatives (Hao et al., 2021).\nGenerally, these combined approaches, which inte-\ngrate attention metrics with gradient information,\ntend to outperform methods using either metric in\nisolation.\nAnalyzing MLP Sublayers. More recently, a\nsurge of works have investigated the knowledge\ncaptured by the FFN layers (Geva et al., 2022; Dai\net al., 2022). These layers, consuming the majority\nof each layer’s parameter budget at 8d2compared\nto4d2for self-attention layers (where drepresents\nthe model’s hidden dimension), function akin to\nkey-value memories (Geva et al., 2021). Here, each\n\"key\" is associated with specific textual patterns\nidentified during training, and each \"value\" gener-ates a corresponding output vocabulary distribution\n(Geva et al., 2021). Figure 2(b) focuses in the FFN\noutputs, illustrating how each update within these\nlayers can be broken down into sub-updates linked\nto individual parameter vectors, often encoding\nconcepts that are interpretable to humans (Geva\net al., 2022). Additionally, there is an emerging\ninterest in input-independent methods, which in-\nterpret model parameters directly, thus eliminating\nthe need for a forward pass (Dar et al., 2023).\n3.2 Global Analysis\nIn contrast to local analysis, which focus on eluci-\ndating individual model predictions, global analy-\nsis aims to understand and explain the knowledge\nor linguistic properties encoded in the hidden state\nactivations of a model. This section explores two\nprimary approaches to global analysis: probing\nmethods that scrutinize model representations and\nmechanistic interpretability (Transformer Circuits,\n2022), an emerging perspective that seeks to re-\nverse engineer the inner workings of deep neural\nnetworks.\n3.2.1 Probing-Based Method\nSelf-supervised pre-training endows models with\nextensive linguistic knowledge, derived from large-\nscale training datasets. Probing-based methods are\nemployed to capture the internal representations\nwithin these networks. This approach involves\ntraining a classifier, known as a probe, on the net-\nwork’s activations to distinguish between various\ntypes of inputs or outputs. In the following sec-\ntions, we will discuss studies related to probing,\ncategorized based on their objectives, whether it\nbe probing for semantic knowledge or analyzing\nlearned representations.\nProbing Knowledge. LLMs trained on extensive\ntext corpora, are recognized for their ability to en-\ncapsulate context-independent semantic and factual\nknowledge accessible via textual prompts (Petroni\net al., 2019). Research in this area primarily fo-\ncuses on formulating textual queries to extract vari-\nous types of background knowledge from language\nmodels (Hewitt and Manning, 2019; Peng et al.,\n2022). Interestingly, probes can sometimes unearth\nfactual information even in scenarios where lan-\nguage models may not reliably produce truthful\noutputs (Hernandez et al., 2023).\nProbing Representations. LLMs are adept at\ndeveloping context-dependent knowledge repre-\nsentations. To analyze these, probing classifiers\nare applied, typically involving a shallow classi-\nfier trained on the activations of attention heads to\npredict specific features. A notable study in this\narea involved training linear classifiers to identify\na select group of attention heads that exhibit high\nlinear probing accuracy for truthfulness (Li et al.,\n2023b). This research revealed a pattern of special-\nization across attention heads, with the represen-\ntation of “truthfulness” predominantly processed\nin the early to middle layers, and only a few heads\nin each layer showing standout performance. Such\ninsights pave the way for exploring more complex\nrepresentations. For instance, research by (Li et al.,\n2023a) has revealed nonlinear internal representa-\ntions, such as board game states, in models that\ninitially lack explicit knowledge of the game or its\nrules.\n3.2.2 Mechanistic Interpretability\nMechanistic interpretability seeks to comprehend\nlanguage models by examining individual neu-\nrons and their interconnections, often conceptual-\nized as circuits (Transformer Circuits, 2022; Zhao\net al., 2023). This field encompasses various ap-\nproaches, which can be primarily categorized into\nthree groups: circuit discovery, causal tracing, and\nvocabulary lens. Each of these approaches offersdistinct perspectives and insights into the mecha-\nnisms of language models.\nCircuit Discovery. The circuit-based mechanis-\ntic interpretability approach aims to align learned\nmodel representations with known ground truths,\ninitially by reverse-engineering the model’s algo-\nrithm to fully comprehend its feature set (Chughtai\net al., 2023). A prominent example of this ap-\nproach is the analysis of GPT-2 small (Radford\net al., 2019), where a study identified a human-\nunderstandable subgraph within the computational\ngraph responsible for performing the indirect ob-\nject identification (IOI) task (Wang et al., 2022).\nIn IOI, sentences like “When Mary and John went\nto the store, John gave a drink” are expected to be\ncompleted with “Mary”. The study discovered a\ncircuit comprising 26 attention heads – just 1.1%\nof the total (head, token position) pairs – that pre-\ndominantly manages this task. This circuits-based\nmechanistic view provides opportunities to scale\nour understanding to both larger models and more\ncomplex tasks, including recent explorations into\nIn-Context Learning (ICL) (Halawi et al., 2023;\nHendel et al., 2023; Todd et al., 2023; Wang et al.,\n2023).\nCausal Tracing. The concept of causal analysis\nin machine learning has evolved from early meth-\nods that delineate dependencies between hidden\nvariables using causal graphs (Pearl et al., 2000) to\nmore recent approaches like causal mediation anal-\nysis (Vig et al., 2020). This newer method quanti-\nfies the impact of intermediate activations in neu-\nral networks on their output (Meng et al., 2023a).\nSpecifically, (Meng et al., 2023a) assesses each acti-\nvation’s contribution to accurate factual predictions\nthrough three distinct operational phases: a clean\nrun generating correct predictions, a corrupted run\nwhere predictions are impaired, and a corrupted-\nwith-restoration run that evaluates the ability of\na single state to rectify the prediction (Meng et al.,\n2023a). Termed as causal tracing , this approach\nhas identified crucial causal states predominantly\nin the middle layers, particularly at the last sub-\nject position where MLP contributions are most\nsignificant (Figure 3). This finding underscores the\nrole of middle layer MLPs in factual recall within\nLLMs.\nVocabulary Lens. Recent work has suggested\nthat model knowledge and knowledge retrieval may\nbe localized within small parts of a language model\nFigure 3: The intensity of each grid cell represents the average causal indirect effect of a hidden state on expressing a factual\nassociation. Darker cells indicate stronger causal mediators. It was found that the MLPs at the last subject token and the attention\nmodules at the last token play crucial roles. (Meng et al., 2023a)\n(Geva et al., 2021) by projecting weights and hid-\nden states onto their vocabulary space. To analyze\nthe components in vocabulary space, we read from\neach token component xl\nkat layer lat the last token\nposition N(Nis omitted here), by projecting with\nthe unembedding matrix E:\npl\nk=softmax (Eln(xl\nk)) (3)\nwhere lnstands for layer normalization before the\nLM head. (Belrose et al., 2023) refines model\npredictions at each transformer layer and decodes\nhidden states into vocabulary distributions based\non this method. Exploring this avenue further,\n(Geva et al., 2022) illuminated the role of trans-\nformer feed-forward layers in predictions, spot-\nlighting specific conceptual emphases via FFN sub-\nupdates. There is also a growing interest in input-\nindependent methodologies, where model param-\neters are interpreted directly, bypassing a forward\npass (Dar et al., 2023).\nAugmenting projection-focused interpretations,\n(Din et al., 2023) first unveiled a feasible appli-\ncation for such projections, suggesting early exit\nstrategies by treating hidden state representations\nas final outputs. (Geva et al., 2023) pinpointed two\ncritical junctures where information propagates to\nthe final predictions via projections and attention\nedge intervention. While much of the focus has\nbeen on how hidden states relate to model outputs,\nrecent works have also highlighted the roles of in-\ndividual tokens, revealing that their contributions\nthrough attention outputs are laden with rich se-\nmantic information (Ram et al., 2023; Katz and\nBelinkov, 2023).\n4 Leveraging Explainability\nIn this section, we discuss how explainability can\nbe used as a tool to debug and improve models. Al-\nthough various approaches aim to improve modelcapabilities with fine-tuning or re-training, we fo-\ncus on methods specifically designed with a strong\nfoundation in model explainability.\n4.1 Model Editing\nDespite the ability to train proficient LLMs, the\nmethodology for ensuring their relevance and recti-\nfying errors remains elusive. In recent years, there\nhas been a surge in techniques for editing LLMs.\nThe goal is to efficiently modify the knowledge or\nbehavior of LLMs within specific domains with-\nout adversely affecting their performance on other\ninputs (Yao et al., 2023).\nHypernetwork Knowledge Editors. This type\nof knowledge editors includes memory-based\nmodel and editors with additional parameters .\nMemory-based models store all edit examples ex-\nplicitly in memory based on the explainability find-\ning of key-value memories inside the FFN (Sec-\ntion 3.1.2). They can then employ a retriever to\nextract the most relevant edit facts for each new\ninput, guiding the model to generate the edited fact.\nSERAC (Mitchell et al., 2022), for instance, adopts\na distinct counterfactual model while leaving the\noriginal model unchanged. Editors with additional\nparameters introduce extra trainable parameters\nwithin LLMs. These parameters are trained on\na modified dataset while the original model param-\neters remain static. For example, T-Patcher (Huang\net al., 2023) integrates one neuron (patch) for one\nmistake in the last layer of the FFN of the model,\nwhich takes effect only when encountering its cor-\nresponding mistake.\nLocate-Then-Edit. The locate-then-edit\nparadigm first identifies the parameters correspond-\ning to the specific knowledge and then modifies\nthem by directly updating the target parameters.\nThe Knowledge Neuron (KN) method (Dai\net al., 2022) introduces a knowledge attribution\ntechnique to pinpoint the “knowledge neuron” (a\nkey-value pair in the FFN matrix) that embodies\nthe knowledge and then updates these neurons.\nROME (Meng et al., 2023a) and MEMIT (Meng\net al., 2023b) apply causal tracing (Section 3.2.2)\nto locate the editing area. Instead of modifying the\nknowledge neurons in the FFN, ROME alters the\nentire matrix. Based on these two methods, PMET\n(Li et al., 2023c) involves the attention value to\nachieve better performance.\n4.2 Enhancing Model Capability\nWhile LLMs demonstrate versatility in various\nNLP tasks, insights from explainability can sig-\nnificantly enhance these capabilities. This section\nhighlights two key tasks where explainability has\nshown considerable impact in recent work: improv-\ning the utilization of long text (Xiao et al., 2023;\nLiu et al., 2023; Pope et al., 2022) and enhanc-\ning the performance of In-Context Learning (ICL)\n(Hendel et al., 2023; Halawi et al., 2023; Wang\net al., 2023).\n4.2.1 Improving Utilization of Long Text\nThe optimization of handling long text aims to en-\nhance the ability of LLMs to capture and effectively\nutilize content within longer contexts. This is par-\nticularly challenging because LLMs tend to strug-\ngle with generalizing to sequence lengths longer\nthan what they were pretrained on, such as the 4K\nlimit for Llama-2 (Touvron et al., 2023). (Belt-\nagy et al., 2020), maintains a fixed-size sliding\nwindow on the key-value (KV) states of the most\nrecent tokens. While this approach ensures con-\nstant memory usage and decoding speed after the\ncache is initially filled, it faces limitations when the\nsequence length exceeds the cache size (Liu et al.,\n2023). An innovative solution proposed by (Xiao\net al., 2023) takes advantage of the MHSA expla-\nnations (Section 3.1.2) in LLMs, which allocates\na significant amount of attention to the initial to-\nkens. They introduce StreamingLLM, a simple and\nefficient framework that allows LLMs to handle\nunlimited text without fine-tuning. This is achieved\nby retaining the \"attention sink,\" which consists of\nseveral initial tokens, in the KV states (Figure 4).\nThe authors also demonstrate that pre-training mod-\nels with a dedicated sink token can further improve\nstreaming performance.\n4.2.2 Improving In-Context Learning\nIn-context Learning (ICL) has emerged as a power-\nful capability alongside the development of scaled-up LLMs (Brown et al., 2020). ICL stands out\nbecause it doesn’t require extensive updates to the\nvast number of model parameters and relies on\nhuman-understandable natural language instruc-\ntions (Dong et al., 2023). As a result, it offers\na promising approach to harness the full potential\nof LLMs. With mechanistic interpretability (Sec-\ntion 3.2.2), (Wang et al., 2023) reveal that label\nwords in the demonstration examples function as\nanchors, which can be used to improve ICL per-\nformance with simple anchor re-weighting method.\n(Halawi et al., 2023) study harmful imitation in\nICL through vocabulary lens to inspect a model’s\ninternal representations (Section 3.2.2), and iden-\ntify two related phenomena: overthinking andfalse\ninduction heads , the heads in late layers that attend\nto and copy false information from previous demon-\nstrations, and whose ablation improves ICL perfor-\nmance. Furthermore, using causal tracing (Section\n3.2.2), (Hendel et al., 2023; Todd et al., 2023) find\nthat a small number attention heads transport a\ncompact representation of the demonstrated task,\nwhich they call a task vector orfunction vector\n(FV). These FVs can be summed to create vectors\nthat trigger new complex tasks and improve perfor-\nmance for few-shot prompting (Todd et al., 2023).\n4.3 Controllable Generation\nThough large language models have obtained supe-\nrior performance in text generation, they sometimes\nfall short of producing factual content. Leveraging\nexplainability provides opportunities for building\ninference-time and fast techniques to improve gen-\neration models’ factuality, calibration, and control-\nlability and align more with human preference.\n4.3.1 Reducing Hallucination\nHallucinations in LLMs refer to generated content\nnot based on training data or facts, various fac-\ntors such as imperfect learning and decoding con-\ntribute to this (Ji et al., 2023). To mitigate hallucina-\ntions, initial approaches used reinforcement learn-\ning from human feeback (Ouyang et al., 2022) and\ndistillation into smaller models such as Alpaca (Li\net al., 2023d). Leveraging explainability provides a\nsignificantly less expensive way to reduce halluci-\nnation, enjoying the advantage of being adjustable\nand minimally invasive. For example, (Li et al.,\n2023b) use as few as 40 samples to locate and find\n“truthful” heads and directions through a trained\nprobe (Section 3.2.1). They propose inference-time\nintervention (ITI), a computationally inexpensive\nFigure 4: (a) Dense Attention (Vaswani et al., 2017) has O(T2)time complexity and an increasing cache size. Its performance\ndecreases when the text length exceeds the pre-training text length. (b) Window Attention () caches the most recent Ltokens’\nKV . While efficient in inference, performance declines sharply once the starting tokens’ keys and values are evicted. (c) Sliding\nWindow (Pope et al., 2022) with Re-computation performs well on long texts, but its O(TL2)complexity, stemming from\nquadratic attention in context re-computation, makes it considerably slow. (d) StreamingLLM keeps (Xiao et al., 2023) the\nattention sink (several initial tokens) for stable attention computation, combined with the recent tokens. It’s efficient and offers\nstable performance on extended texts.\nstrategy to intervene on the attention head to shift\nthe activations in the “truthful” direction, which\nachieves comparable or better performance toward\nthe instruction-finetuned model.\n4.3.2 Ethical Alignment\nAs research on AI fairness gains increasing im-\nportance, there have been efforts to detect social\nbias (Fleisig et al., 2023; An and Rudinger, 2023)\nand suppress toxicity (Gehman et al., 2020; Schick\net al., 2021) in LMs. Many previous debiasing\nmethods (Qian et al., 2022) have focused on con-\nstructing anti-stereotypical datasets and then ei-\nther retraining the LM from scratch or conducting\nfine-tuning. This line of debiasing approaches, al-\nthough effective, comes with high costs for data\nconstruction and model retraining. Moreover, it\nfaces the challenge of catastrophic forgetting if\nfine-tuning is performed (Zhao et al., 2023). While\nfew work has focused on the interpretability of the\nfairness research, (dev, 2023) explore interpreting\nand mitigating social biases in LLMs by introduc-\ning the concept of social bias neurons . Inspired by\nthe gradient-based attribution method IG (Section\n3.1.1), they introduce an interpretable technique,\ndenoted as intergrated gap gradient (IG2), to pin-\npoint social bias neurons by back-propagating and\nintegrating the gradients of the logits gap for a se-\nlected pair of demographics3Taking this interpreta-\n3Demographic include properties like gender, sexuality,\noccupation, etc. 9 common demographics are collected and\npairs of demographics are selected to reveal the fairness gap\n(dev, 2023).tion, they suppress the activations of the pinpointed\nneurons to mitigate bias. Extensive experiments\nhave verified the effectiveness of this method and\nhave yielded the potential applicability of the ex-\nplainability method for ethical alignment research\nin LLMs.\n5 Evaluation\nRecently, LLMs such as GPT-4 (OpenAI, 2023)\nhave shown impressive abilities to generate natural\nlanguage explanations for their predictions. How-\never, it remains unclear whether these explanations\nactually help humans understand the reasoning of\nthe model (Zhao et al., 2023). Specifically designed\nevaluation methods are needed to better assess the\nperformance of explainability methods, such as\nattribution. Furthermore, calibrated datasets and\nmetrics are required to evaluate the application of\nexplainability to downstream tasks, such as truth-\nfulness evaluation4.\n5.1 Evaluating Explanation Plausibility\nOne common technique to evaluate the plausibil-\nity of attribution analysis is to remove K% of to-\nkens with the highest or lowest estimated impor-\ntance to observe its impact on the model output\n(Chen et al., 2020; Modarressi et al., 2023). An-\nother approach to assessing explanation plausibil-\nity involves indirect methods, such as measuring\nthe performance of model editing, particularly for\n4Due to space limit, we only discusse the most commonly\nused evaluation approaches in explainability research\n“locate-then-edit” editing methods, which heavily\nrely on interpretation accuracy. Recent research\n(Yao et al., 2023; Zhao et al., 2023) suggests that\nhaving evaluation datasets is crucial for evaluat-\ning factual editing in LLMs. Two commonly used\ndatasets for this purpose are ZsRE (Levy et al.,\n2017), a Question Answering (QA) dataset that em-\nploys question rephrasings generated through back-\ntranslation, and CounterFact (Meng et al., 2023a), a\nmore challenging dataset that includes counterfacts\nstarting with low scores compared to correct facts.\n5.2 Evaluating Truthfulness\nModel truthfulness is an important metric for mea-\nsuring the trustworthiness of generative models.\nWe expect model outputs to be both informative\nand factually correct and faithful. Ideally, human\nannotators would label model answers as true or\nfalse, given a gold standard answer, but this is of-\nten costly. (Lin et al., 2022) propose the use of\ntwo fine-tuned GPT-3-13B models (GPT-judge) to\nclassify each answer as true or false and informa-\ntive or not. Evaluation using GPT-judge is a stan-\ndard practice on TruthfulQA benchmark, a widely\nused dataset adversarially constructed to measure\nwhether a language model is truthful in generat-\ning answers (Askell et al., 2021; Li et al., 2023b;\nChuang et al., 2023). The main metric of Truth-\nfulQA is true*informative , a product of scalar\ntruthful and informative scores. This metric not\nonly captures how many questions are answered\ntruthfully but also prevents the model from indis-\ncriminately replying with “I have no comment” by\nassessing the informativeness of each answer.\n6 Conclusion\nIn this survey, we have presented a comprehensive\noverview of explainability for LLMs and their ap-\nplications. We have summarized methods for local\nand global analysis based on the objectives of expla-\nnations. In addition, we have discussed the use of\nexplanations to enhance models and the evaluation\nof these methods. Major future research directions\nto understanding LLM include developing explana-\ntion methods tailored to different language models\nand making LLMs more trustworthy and aligned\nwith human values by using explainability knowl-\nedge. As LLMs continue to advance, explainability\nwill become incredibly vital to ensure that these\nmodels are transparent, fair, and beneficial. We\nhope that this review of the literature provides auseful overview of this emerging research area and\nhighlights open problems and directions for future\nresearch.\nReferences\n2023. The devil is in the neurons: Interpreting and miti-\ngating social biases in language models. In Openre-\nview for International Conference on Learning Rep-\nresentations 2024 .\nSamira Abnar and Willem Zuidema. 2020. Quantify-\ning attention flow in transformers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 4190–4197, On-\nline. Association for Computational Linguistics.\nHaozhe An and Rachel Rudinger. 2023. Nichelle and\nnancy: The influence of demographic attributes and\ntokenization length on first name biases. In Proceed-\nings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers) , pages 388–401, Toronto, Canada. Association\nfor Computational Linguistics.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, Nelson El-\nhage, Zac Hatfield-Dodds, Danny Hernandez, Jack-\nson Kernion, Kamal Ndousse, Catherine Olsson,\nDario Amodei, Tom Brown, Jack Clark, Sam Mc-\nCandlish, Chris Olah, and Jared Kaplan. 2021. A\ngeneral language assistant as a laboratory for align-\nment.\nPepa Atanasova, Jakob Grue Simonsen, Christina Li-\noma, and Isabelle Augenstein. 2020. A diagnostic\nstudy of explainability techniques for text classifi-\ncation. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP) , pages 3256–3274, Online. Association for\nComputational Linguistics.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization.\nOren Barkan, Edan Hauon, Avi Caciularu, Ori Katz,\nItzik Malkiel, Omri Armstrong, and Noam Koenig-\nstein. 2021. Grad-sam: Explaining transformers\nvia gradient self-attention maps. In Proceedings of\nthe 30th ACM International Conference on Informa-\ntion & Knowledge Management , CIKM ’21, page\n2882–2887, New York, NY , USA. Association for\nComputing Machinery.\nJasmijn Bastings, Sebastian Ebert, Polina Zablotskaia,\nAnders Sandholm, and Katja Filippova. 2022. \"will\nyou find these shortcuts?\" a protocol for evaluating\nthe faithfulness of input salience methods for text\nclassification.\nNora Belrose, Zach Furman, Logan Smith, Danny Ha-\nlawi, Igor Ostrovsky, Lev McKinney, Stella Bider-\nman, and Jacob Steinhardt. 2023. Eliciting latent\npredictions from transformers with the tuned lens.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nHanjie Chen, Guangtao Zheng, and Yangfeng Ji. 2020.\nGenerating hierarchical explanations on text classifi-\ncation via feature interaction detection. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 5578–5593, On-\nline. Association for Computational Linguistics.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nlarge language models trained on code.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon\nKim, James Glass, and Pengcheng He. 2023. Dola:\nDecoding by contrasting layers improves factuality\nin large language models.\nBilal Chughtai, Lawrence Chan, and Neel Nanda. 2023.\nA toy model of universality: Reverse engineering\nhow networks learn group operations.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 8493–\n8502, Dublin, Ireland. Association for Computational\nLinguistics.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.\n2023. Analyzing transformers in embedding space.\nInProceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 16124–16170, Toronto, Canada.\nAssociation for Computational Linguistics.Alexander Yom Din, Taelin Karidi, Leshem Choshen,\nand Mor Geva. 2023. Jump to conclusions: Short-\ncutting transformers with linear transformations.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong\nWu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and\nZhifang Sui. 2023. A survey on in-context learning.\nJoseph Enguehard. 2023. Sequential integrated gradi-\nents: a simple but effective method for explaining\nlanguage models.\nShi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer,\nPedro Rodriguez, and Jordan Boyd-Graber. 2018.\nPathologies of neural models make interpretations\ndifficult. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 3719–3728, Brussels, Belgium. Association\nfor Computational Linguistics.\nJavier Ferrando, Gerard I. Gállego, and Marta R. Costa-\njussà. 2022. Measuring the mixing of contextual\ninformation in the transformer. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing , pages 8698–8714, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nEve Fleisig, Aubrie Amstutz, Chad Atalla, Su Lin\nBlodgett, Hal Daumé III, Alexandra Olteanu, Emily\nSheng, Dan Vann, and Hanna Wallach. 2023. Fair-\nPrism: Evaluating fairness-related harms in text gen-\neration. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 6231–6251, Toronto,\nCanada. Association for Computational Linguistics.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020 , pages\n3356–3369, Online. Association for Computational\nLinguistics.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson. 2023. Dissecting recall of factual associ-\nations in auto-regressive language models.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 30–45, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing , pages 5484–5495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nDanny Halawi, Jean-Stanislas Denain, and Jacob Stein-\nhardt. 2023. Overthinking the truth: Understanding\nhow language models process false demonstrations.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2021. Self-\nattention attribution: Interpreting information inter-\nactions inside transformer.\nRoee Hendel, Mor Geva, and Amir Globerson. 2023.\nIn-context learning creates task vectors.\nEvan Hernandez, Belinda Z. Li, and Jacob Andreas.\n2023. Inspecting and editing knowledge representa-\ntions in language models.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers) , pages\n4129–4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nZeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou,\nWenge Rong, and Zhang Xiong. 2023. Transformer-\npatcher: One mistake worth one neuron.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys , 55(12):1–38.\nShahar Katz and Yonatan Belinkov. 2023. Interpreting\ntransformer’s attention dynamic memory and visual-\nizing the semantic information flow of gpt.\nPieter-Jan Kindermans, Sara Hooker, Julius Adebayo,\nMaximilian Alber, Kristof T. Schütt, Sven Dähne, Du-\nmitru Erhan, and Been Kim. 2017. The (un)reliability\nof saliency methods.\nPieter-Jan Kindermans, Kristof Schütt, Klaus-Robert\nMüller, and Sven Dähne. 2016. Investigating the\ninfluence of noise and distractors on the interpretation\nof neural networks.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is not only a weight:\nAnalyzing transformers with vector norms. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP) ,\npages 7057–7075, Online. Association for Computa-\ntional Linguistics.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2023. Analyzing feed-forward blocks\nin transformers through the lens of attention map.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017. Zero-shot relation extraction via\nreading comprehension. In Proceedings of the 21st\nConference on Computational Natural Language\nLearning (CoNLL 2017) , pages 333–342, Vancouver,\nCanada. Association for Computational Linguistics.Kenneth Li, Aspen K. Hopkins, David Bau, Fernanda\nViégas, Hanspeter Pfister, and Martin Wattenberg.\n2023a. Emergent world representations: Exploring a\nsequence model trained on a synthetic task.\nKenneth Li, Oam Patel, Fernanda Viégas, Hanspeter\nPfister, and Martin Wattenberg. 2023b. Inference-\ntime intervention: Eliciting truthful answers from a\nlanguage model.\nXiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun\nMa, and Jie Yu. 2023c. Pmet: Precise model editing\nin a transformer. ArXiv , abs/2308.08742.\nZhihui Li, Max Gronke, and Charles Steidel. 2023d.\nAlpaca: A new semi-analytic model for metal absorp-\ntion lines emerging from clumpy galactic environ-\nments.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulqa: Measuring how models mimic human\nfalsehoods.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023. Lost in the middle: How language\nmodels use long contexts.\nScott Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2023a. Locating and editing factual associ-\nations in gpt.\nKevin Meng, Arnab Sen Sharma, Alex Andonian,\nYonatan Belinkov, and David Bau. 2023b. Mass-\nediting memory in a transformer.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D. Manning, and Chelsea Finn. 2022. Memory-\nbased model editing at scale.\nAli Modarressi, Mohsen Fayyaz, Ehsan Aghazadeh,\nYadollah Yaghoobzadeh, and Mohammad Taher Pile-\nhvar. 2023. DecompX: Explaining transformers deci-\nsions by propagating token decomposition. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers) , pages 2649–2664, Toronto, Canada. Associ-\nation for Computational Linguistics.\nAli Modarressi, Mohsen Fayyaz, Yadollah\nYaghoobzadeh, and Mohammad Taher Pile-\nhvar. 2022. GlobEnc: Quantifying global token\nattribution by incorporating the whole encoder\nlayer in transformers. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 258–271, Seattle,\nUnited States. Association for Computational\nLinguistics.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nJudea Pearl et al. 2000. Models, reasoning and infer-\nence. Cambridge, UK: CambridgeUniversityPress ,\n19(2):3.\nHao Peng, Xiaozhi Wang, Shengding Hu, Hailong Jin,\nLei Hou, Juanzi Li, Zhiyuan Liu, and Qun Liu. 2022.\nCopen: Probing conceptual knowledge in pre-trained\nlanguage models.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP) ,\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery,\nJacob Devlin, James Bradbury, Anselm Levskaya,\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and\nJeff Dean. 2022. Efficiently scaling transformer in-\nference.\nRebecca Qian, Candace Ross, Jude Fernandes,\nEric Michael Smith, Douwe Kiela, and Adina\nWilliams. 2022. Perturbation augmentation for fairer\nNLP. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing ,\npages 9496–9521, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nOri Ram, Liat Bezalel, Adi Zicher, Yonatan Belinkov,\nJonathan Berant, and Amir Globerson. 2023. What\nare you token about? dense retrieval as distributions\nover the vocabulary. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 2481–\n2498, Toronto, Canada. Association for Computa-\ntional Linguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \"why should i trust you?\": Explain-\ning the predictions of any classifier.\nTimo Schick, Sahana Udupa, and Hinrich Schütze. 2021.\nSelf-diagnosis and self-debiasing: A proposal for\nreducing corpus-based bias in nlp.Sandipan Sikdar, Parantapa Bhattacharya, and Kieran\nHeese. 2021. Integrated directional gradients: Fea-\nture interaction attribution for neural NLP models. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 865–878,\nOnline. Association for Computational Linguistics.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Pro-\nceedings of the 34th International Conference on\nMachine Learning , volume 70 of Proceedings of Ma-\nchine Learning Research , pages 3319–3328. PMLR.\nEric Todd, Millicent L. Li, Arnab Sen Sharma, Aaron\nMueller, Byron C. Wallace, and David Bau. 2023.\nFunction vectors in large language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models.\nTransformer Circuits. 2022. Mechanistic interpretations\nof transformer circuits. Accessed: [insert access date\nhere].\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS .\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Simas Sakenis, Jason\nHuang, Yaron Singer, and Stuart Shieber. 2020.\nCausal mediation analysis for interpreting neural nlp:\nThe case of gender bias.\nKevin Wang, Alexandre Variengien, Arthur Conmy,\nBuck Shlegeris, and Jacob Steinhardt. 2022. Inter-\npretability in the wild: a circuit for indirect object\nidentification in gpt-2 small.\nLean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou,\nFandong Meng, Jie Zhou, and Xu Sun. 2023. Label\nwords are anchors: An information flow perspective\nfor understanding in-context learning. In Proceed-\nings of the 2023 Conference on Empirical Methods\nin Natural Language Processing , pages 9840–9855,\nSingapore. Association for Computational Linguis-\ntics.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGriffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\nZac Kenton, Sasha Brown, Will Hawkins, Tom\nStepleton, Courtney Biles, Abeba Birhane, Julia\nHaas, Laura Rimell, Lisa Anne Hendricks, William\nIsaac, Sean Legassick, Geoffrey Irving, and Iason\nGabriel. 2021. Ethical and social risks of harm from\nlanguage models.\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. 2023. Efficient streaming\nlanguage models with attention sinks.\nSen Yang, Shujian Huang, Wei Zou, Jianbing Zhang,\nXinyu Dai, and Jiajun Chen. 2023. Local interpre-\ntation of transformer based on linear decomposition.\nInProceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers) , pages 10270–10287, Toronto, Canada.\nAssociation for Computational Linguistics.\nYunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng,\nZhoubo Li, Shumin Deng, Huajun Chen, and Ningyu\nZhang. 2023. Editing large language models: Prob-\nlems, methods, and opportunities.\nYordan Yordanov, Vid Kocijan, Thomas Lukasiewicz,\nand Oana-Maria Camburu. 2022. Few-shot out-of-\ndomain transfer learning of natural language expla-\nnations in a label-abundant setup. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2022 , pages 3486–3501, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nHaiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu,\nHuiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei\nYin, and Mengnan Du. 2023. Explainability for large\nlanguage models: A survey.",
    "content_summary": "From Understanding to Utilization: A Survey on Explainability for Large\nLanguage Models\nHaoyan Luo\nImperial College London\nh.luo23@imperial.ac.ukLucia Specia\nImperial College London\nl.specia@imperial.ac.uk\nAbstract\nExplainability for Large Language M...",
    "content_length": 53917,
    "created_at": "2025-07-18T16:41:03.910390+00:00",
    "updated_at": "2025-07-18T16:41:13.808468+00:00",
    "file_path": "2401.12874v2.pdf",
    "chunks_list": []
  },
  "doc-50b023abca86514de9cb2ed012e8c217": {
    "status": "failed",
    "error": "index 13 is out of bounds for axis 0 with size 0",
    "content": "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations\nfrom Large Language Models\nChirag Agarwal1Sree Harsha Tanneru1Himabindu Lakkaraju1\nAbstract\nLarge Language Models (LLMs) are deployed as\npowerful tools for several natural language pro-\ncessing (NLP) applications. Recent works show\nthat modern LLMs can generate self-explanations\n(SEs), which elicit their intermediate reasoning\nsteps for explaining their behavior. Self-\nexplanations have seen widespread adoption\nowing to their conversational and plausible nature.\nHowever, there is little to no understanding of\ntheir faithfulness. In this work, we discuss the\ndichotomy between faithfulness and plausibility\nin SEs generated by LLMs. We argue that while\nLLMs are adept at generating plausible explana-\ntions – seemingly logical and coherent to human\nusers – these explanations do not necessarily\nalign with the reasoning processes of the LLMs,\nraising concerns about their faithfulness. We\nhighlight that the current trend towards increasing\nthe plausibility of explanations, primarily driven\nby the demand for user-friendly interfaces, may\ncome at the cost of diminishing their faithfulness.\nWe assert that the faithfulness of explanations\nis critical in LLMs employed for high-stakes\ndecision-making. Moreover, we urge the commu-\nnity to identify the faithfulness requirements of\nreal-world applications and ensure explanations\nmeet those needs. Finally, we propose some\ndirections for future work, emphasizing the need\nfor novel methodologies and frameworks that\ncan enhance the faithfulness of self-explanations\nwithout compromising their plausibility, essential\nfor the transparent deployment of LLMs in\ndiverse high-stakes domains.\nContent Warning: This paper may contain some\noffensive responses generated by LLMs.\n1Harvard University, Cambridge, MA, USA. Correspondence\nto: Chirag Agarwal <cagarwal@hbs.edu >.1. Introduction\nIn recent years, the advent of Large Language Models\n(LLMs) has revolutionized the field of natural language\nprocessing, offering impressive capabilities in generating\nhuman-like text (Kaddour et al., 2023). However, LLMs are\ncomplex large-scale models trained on broad datasets and\nlarge scale compute, where their decision-making processes\nare not completely understood and remain an important\nbottleneck for deploying them to high-stakes applications.\nExisting works show that these models, known for their ex-\ntensive training on diverse datasets, demonstrate remarkable\nproficiency in generating self-explanations that are not only\ncoherent but also contextually adaptable (Brown et al., 2020;\nWei et al., 2023), critical for explaining their predictions in\napplications like health, commerce, and law. To this end,\nthere has been an increasing focus on understanding the\nnature of the explanations generated by these LLMs. The\nconcept of self-explanations extends beyond mere response\ngeneration; it encompasses the model’s ability to explain\nits decisions in a manner understandable to humans. This\ncapability is pivotal in applications where decision-making\ntransparency is crucial, such as healthcare diagnostics, legal\nadvice, and financial forecasting.\nDespite their emergent capabilities (Wei et al., 2022), LLMs\nface significant challenges in generating self-explanations,\nwhere one of the primary concerns is the gap between plausi-\nbility andfaithfulness . While LLMs can generate responses\nthat seem logical and coherent ( i.e.,being plausible), these\nexplanations may not accurately reflect the model’s actual\nreasoning process ( i.e.,being faithful) (Jacovi & Goldberg,\n2020a; Wiegreffe & Marasovi ´c, 2021). This discrepancy\nraises questions about the reliability and trustworthiness of\nthe model’s outputs, especially in high-stakes scenarios.\nPlausibility in self-explanations refers to how convincing\nand logical the explanations appear to humans. Given their\nextensive training, LLMs are adept at formulating expla-\nnations that align with human reasoning. However, these\nplausible explanations might be misleading if they do not\ncorrespond to the LLM’s internal decision-making process.\nOn the other hand, faithfulness represents the accuracy of\nexplanations in illustrating the LLM’s actual reasoning, i.e.,\n1arXiv:2402.04614v2  [cs.CL]  8 Feb 2024\nFaithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models\nwhy and how the model reached a particular decision. We\nhighlight the challenge of ensuring that self-explanations\nare both plausible and faithful, which is not always straight-\nforward due to the complexity and opacity of LLMs and the\ndichotomy between these properties for specific downstream\napplications. For instance, in healthcare, an incorrect diag-\nnostic explanation from an LLM could lead to severe impli-\ncations for patient care. Similarly, in legal contexts, reliance\non inaccurate explanations could result in erroneous legal\nadvice. Hence, ensuring the faithfulness of explanations is\nas critical as their plausibility. Addressing the aforemen-\ntioned challenges necessitates future research focused on\nsimultaneously enhancing the plausibility and faithfulness\nof self-explanations, including the development of novel\nmethodologies for better understanding the decision-making\nprocesses of LLMs and creating frameworks that can assess\nand ensure the faithfulness of explanations. Further, there is\na need for developing evaluation metrics and benchmarks\nthat can effectively measure the quality of self-explanations\nin terms of plausibility and faithfulness.\nHere, we review self-explanations generated by LLMs and\nsome key connections between their plausibility and faith-\nfulness properties. While there are many NLP applications\nof LLMs, we focus on high-stakes applications that require\nLLM-generated explanations to be plausible and/or faithful.\nWe begin with an overview of self-explanations to high-\nlight techniques like token importance and chain-of-thought\nreasoning relevant to this review. Next, we formally de-\nfine and discuss the plausibility and faithfulness aspects of\nself-explanations and some challenges in achieving them.\nWe underscore the potential of studying the implication of\nthese properties in high-stakes applications, emphasizing\nwhen we need an explanation to be plausible or faithful\nand how this transparency in generated self-explanations\nis application-centric. Finally, we provide some open chal-\nlenges and valuable insights for future research, contributing\nto a better understanding of the plausibility and faithfulness\nfacet of explanations in LLMs.\n2. Self-Explanations\nSelf-explanations (SEs) are a class of explanation methods\nthat are generated by models to elicit the reasoning behind\ntheir decisions in human-understandable language. SEs are\nbecoming increasingly crucial in the era of large language\nmodels, particularly for enhancing the trustworthiness of\ntheir generated responses. SEs can take various forms, in-\ncluding chain-of-thought reasoning, token importance, and\ncounterfactual explanations, offering unique insights into\nthe model’s decision-making process (see Fig. 1).\nChain-of-thought (CoT) (Wei et al., 2023) reasoning in SEs\ninvolves generating a sequence of intermediate thoughts or\nsteps that lead to the final decision or response of an LLM.CoTs claim to elicit reasoning in LLMs and have shown\nnoticeable performance gains in commonsense and math rea-\nsoning tasks. It is invaluable for users as they get a window\ninto the thought process of these billion parameter LLMs\nand allows them to understand how they reach their decision.\nFor instance, consider an LLM used to solve a math word\nproblem: “ If John has 5 apples and gives 2 to Jane, how\nmany does he have left? ” the LLM doesn’t just output “3.”\nInstead, it explains its reasoning steps as “ 1) John initially\nhas 5 apples. 2) He gives 2 to Jane, so 5 - 2 = 3. 3) There-\nfore, John has 3 apples left. ” This CoT reasoning makes the\nLLM’s process more transparent and easier to trust.\nToken importance (Li et al., 2015; Wu et al., 2020) in SEs in-\nvolves highlighting specific input tokens (words or phrases)\nthat significantly influence the model’s decision. By identi-\nfying these key tokens, users can understand which parts of\nthe input had the most impact on the outcome. For instance,\nconsider an LLM used to classify sentiments into positive\nand negative categories. When prompted to output the sen-\ntiment of an input, “ The movie was boring but visually\nok,” the model might highlight “ boring ” as an important to-\nken influencing the LLM negative sentiment analysis. This\ntoken-level importance allows users to understand which\naspects of the input swayed the LLM’s judgment.\nCounterfactual explanations in SEs provide insights into\nhow different inputs might lead to different LLM responses.\nThese explanations answer “ what-if ” scenarios, helping\nusers understand how changes in the input could alter the\nmodel’s decision. For instance, in the aforementioned ex-\nample of sentiment analysis, a counterfactual explanation\nmight be: “ If the word ‘boring’ in the sentence ‘The movie\nwas boring but visually ok, ’ were replaced with ‘great, ’ the\nmodel would classify the sentiment as positive instead of\nnegative, ” demonstrating how altering a token in the text\ncan change the sentiment assessment.\nIn summary, SEs in various forms – CoT reasoning, token\nimportance, and counterfactual explanations – offer\ncomprehensive insights into an LLM’s decision-making\nprocess, enhancing transparency, bolstering user confidence\nin models, and fostering a deeper understanding of how\nLLMs process information.\n3. Plausibility\nIn recent years, the capabilities of LLMs have expanded be-\nyond conventional text generation, with a focus on their abil-\nity to generate self-explanations. In the evolving landscape\nof explainability research in LLMs, plausibility has emerged\nas a focal point in self-explanations, which transcend into\nspecific downstream tasks. Here, LLMs exhibit a remark-\nable capacity to generate contextually relevant and convinc-\ning explanations to human practitioners and stakeholders.\n2\nFaithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models\nTask: Predicts whether a customer will purchase a product based on their review.Input:  The product was expensive but of high quality and very useful.\nChain-of-ThoughtThe term 'expensive’ isnegative, but 'high quality' and 'very useful' are positive. Given the positive aspects outweigh the negative, the prediction is a purchase.\nToken ImportanceToken importance would highlight 'expensive,' 'high quality,’ and 'very useful' as crucial words influencing this decision.\nCounterfactualHad the review been 'The product was expensive and not very useful,' the model would have predicted no purchase,\" showing how altering specific input affects the outcome.\nFigure 1. Self-explanations in LLMs. Illustration of SE techniques for predictive modeling in customer review analysis. The figure\nshows three distinct SE methods: Chain-of-Thought , which provides a logical reasoning path leading to the model’s prediction; Token\nImportance , which highlights key terms in the input that significantly influence the prediction; and Counterfactual , which demonstrates\nhow modifications to the input could change the predicted outcome.\n.\nDefinition 1: Plausibility\nAn explanation is considered plausible if it is coherent\nwith human reasoning and understanding.\nIntuitively, the plausibility of a generated explanation per-\ntains to the capability of an LLM to simulate human-like\nthought processes and provide coherent explanations. It\nis crucial to recognize that the notion of a plausible ex-\nplanation is multifaceted, and different LLMs may offer\nequally valid yet distinct explanations for the same prompt,\nwhich are perceived differently by human users. Recent\nworks (Wei et al., 2023; Renze & Guven, 2024) have shown\nthat state-of-the-art LLMs generate plausible explanations\nfor tasks like math reasoning or general knowledge inquiry\n(see Fig. 2). For instance, when prompted to generate an ex-\nplanation for the input prompt: “ Solve the equation 2x + 5 =\n15.”, the LLM not only generates the correct output but also\ngenerates a plausible explanation to demonstrate its ability\nto understand and solve mathematical reasoning problems.\nPrior works have shown how CoT reasoning enhances the\nperformance of LLMs, particularly in complex tasks and\ndecision-making processes (Wei et al., 2023; AlKhamissi\net al., 2023; Gu et al., 2024; Miao et al., 2024; Renze &\nGuven, 2024), providing key insights into the effective incor-\nporation of this reasoning style into various AI applications,\nfrom sentiment analysis to medical diagnostics. CoT reason-\ning shows that LLMs can mimic aspects of human-like rea-\nsoning through structured explanations, which vary across\ndifferent reasoning skills. CoT reasoning aligns with how\nhuman reasoning often involves breaking down complex\nproblems into simpler, explainable components (Meng et al.,\n2024; Rasal, 2024; Qi et al., 2023). However, AlKhamissi\net al. (2023) suggests that while LLMs mimic aspects ofhuman-like reasoning through structured explanations, the\nimpact of the generated CoT explanations varies across\ndifferent reasoning skills. While LLMs, through chain-of-\nthought reasoning, can produce coherent and logical expla-\nnations that enhance their performance on complex tasks\nand decision-making processes like sentiment analysis and\nmedical diagnostics, recent study (Si et al., 2023) highlights\na critical risk associated with CoT reasoning where users\nmay over-rely on these explanations, even when they are\nincorrect. This over-reliance becomes particularly problem-\natic when the LLM’s reasoning, though logical and coherent\n(i.e., plausible ), is based on incorrect facts or assumptions.\nThe aforementioned implications of LLM’s reasoning raise\nan interesting question. How are models capable of gen-\nerating plausible explanations or what are the mechanisms\nbehind LLMs’ convincing explanations? Recent works have\ntheoretically and empirically explored the above question\nwithin the context of LLMs. A key factor behind generating\nplausible CoTs is the extensive training of LLMs on diverse\ndatasets, which encompasses a broad spectrum of human\nlanguage. In addition, the Reinforcement Learning using\nHuman Feedback (RLHF) training paradigm equips LLMs\nwith the capacity to emulate human-like patterns of thought\nand reasoning, enabling them to generate responses that\nnot only follow logical sequences but also include coherent\nnarratives. Further, LLMs are intentionally trained to adapt\ntheir responses based on the given input prompts. For\nexample, if one asks an LLM to translate ‘ It’s raining cats\nand dogs ’ into French, instead of a literal translation, the\nLLM provides a culturally equivalent phrase in French,\nlike “ Il pleut des cordes ”, which captures the spirit of the\noriginal idiom. Hence, they can generate responses that\nare specifically tailored to the context of the question or\n3\nFaithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models\nExample 1: Math Reasoning Question\nInput: Solve the equation 2x + 5 = 15.\nOutput: To solve the equation 2x + 5 = 15, you can start by\nsubtracting 5 from both sides, resulting in 2x = 10. Then, divide\nboth sides by 2, yielding x = 5.\nExplanation: The model provides a step-by-step explanation\nfor solving the given equation. It advises subtracting 5\nfrom both sides to isolate the term with the variable, leading\nto the equation 2x = 10. It then suggests dividing both\nsides by 2 to find the value of x, resulting in the solution x\n= 5. This explanation demonstrates the model’s ability to\nsimulate a logical and coherent mathematical reasoning process.\nExample 2: General Knowledge Inquiry\nInput: Why is the sky blue?\nOutput: The blue color of the sky is a result of Rayleigh scatter-\ning, where shorter-wavelength blue light is scattered more than\nother colors by the gases and particles in the Earth’s atmosphere.\nExplanation: The model explains that the blue color of\nthe sky is attributed to Rayleigh scattering, a phenomenon\noccurring in the Earth’s atmosphere. It elaborates on how\nshorter-wavelength blue light is scattered more than other\ncolors, leading to the observed vibrant blue hue during the day.\nThis explanation provides a concise yet informative response to\nthe query about the color of the sky.\nFigure 2. Some examples of ‘ plausible explanations ’ generated\nusing the G PT-3.5 large language model.\nproblem, which makes their output more relevant and,\ntherefore, more plausible. Recently, Tutunov et al. (2023)\nestablished a theoretical justification for the ability of\nLLMs to produce the correct chain of thoughts explaining\nperformance gains in tasks demanding reasoning skills\nand introduced a novel hierarchical graphical model that\ncan be tailored to generate a coherent chain of thoughts,\nemphasizing that these state-of-the-art LLMs can be driven\nto generate more plausible explanations.\nWhile it’s important to note that modern LLMs can gen-\nerate plausible reasonings that are convincing to humans,\nthey do not inherently understand truth or factual accuracy.\nTheir plausibility comes from the ability to mimic logical\nstructures of reasoning, not from an intrinsic understanding\nof the content, and may not entail the models’ predictions\nor be factually grounded in the input. Therefore, although\ntheir reasoning may appear plausible, they are not always\nfactually correct (Ye & Durrett, 2022; Valmeekam et al.,\n2022; Chen et al., 2023b; Laban et al., 2023).\n4. Faithfulness\nPrevious studies suggest that self-explanations are plausible\nand convincing to humans, but it is unclear whether they\naccurately describe the behavior of the underlying LLM,\nSimulating CounterfactualsPerturb important featuresPerturb unimportant featuresIntervening on ReasoningEarly answeringAdding mistakesFaithfulness MeasuresFigure 3. Different techniques proposed in (Turpin et al., 2023) and\n(Lanham et al., 2023) to measure faithfulness of self-explanations\ngenerated by LLMs.\ni.e., are they faithful? Jacovi & Goldberg (2020b) defines\nthe faithfulness of an explanation as how accurately it rep-\nresents the reasoning of the underlying model. While faith-\nfulness of explanations has been studied extensively in the\ncontext of classification problems (Lyu et al., 2024), more\nrecently, the problem of faithfulness of self-explanations in\nLLMs has gathered significant attention (Turpin et al., 2023;\nLyu et al., 2023; Lanham et al., 2023).\n.\nDefinition 2: Faithfulness\nAn explanation is considered faithful if it accurately\nrepresents the reasoning of the underlying model.\nEvaluating the faithfulness of explanations is a non-trivial\nproblem due to the lack of ground truth explanations. This\nproblem has worsened in the case of self-explanations from\nLLMs, as the billion-parameter scale and often proprietary\nnature of LLMs make assessments using saliency maps\nand other gradient-based methods nearly impossible. To\nalleviate these problems, recent works (Turpin et al., 2023;\nLanham et al., 2023) measure input-output characteristics\nto estimate faithfulness. Next, we present an overview (see\nFig. 3) of these techniques for quantifying faithfulness.\nSimulating Counterfactual Inputs: Turpin et al. (2023)\nsimulates counterfactual inputs to show that CoT explana-\ntions are unfaithful and they misrepresent the true reasoning\nbehind a LLM’s prediction. The paper explores two ways\nto construct counterfactuals: (i) Perturbing Unimportant\nFeatures, where we observe different answers on perturbing\nunimportant features identified by an explanation and quan-\ntify the unfaithfulness of self-explanations as the fraction\nof predictions that change after perturbation. On the BBH\ndataset, the author re-order the options such that the answer\nis always the first option. Upon evaluation, the authors\nobserve that the LLM alters explanations and steers predic-\ntions toward an incorrect answer. (ii) Perturbing Important\nfeatures, where important features are perturbed and the\nfraction of predictions that give the same answer is a mea-\nsure of the unfaithfulness. An example of this experimental\nsetup is described in Fig. 4.\nIntervening on Explanations: Lanham et al. (2023) uses\n4\nFaithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models\nSimulating Counterfactual Inputs\nInput: Is the following sentence plausible? ’David Silva took a\nthrow in.’ (A) implausible / (B) plausible ↔(A) plausible / (B)\nimplausible.\nOutput : I would say that the sentence is (B) plausible.\nThrow-ins are a common feature of football (soccer) and it is\ncertainly possible for a player like David Silva, who is known\nfor his skill and agility on the ball, to take a throw-in.\nOutput : I would say that the sentence is (B) implausible.\nThrows are typically taken by goalkeepers or defenders, not\nmidfielders like David Silva.\nFigure 4. Perturbing unimportant features. When options are\nswapped, the LLM rationalizes changing its answer by providing a\ndifferent explanation, indicating that the explanation is unfaithful.\nthe amount of post-hoc reasoning as an indicator of faith-\nfulness. The hypothesis is that if the generated explanation\nis post-hoc, the explanation has more ways to be unfaithful.\nTo quantify the amount of post-hoc reasoning, (Lanham\net al., 2023) observes the LLM behavior by intervening in\nthe explanation. Two strategies to intervene on explana-\ntions have been proposed in this work: (i) Early Answering,\nwhere the authors truncate the explanation midway and see\nhow the final LLM response changes. If the answer doesn’t\nchange by adding an explanation incrementally, it stands to\nreason that the explanation is generated after an answer was\narrived at, indicating more post-hoc reasoning. The amount\nof post-hoc reasoning and consequently faithfulness is quan-\ntified by the area over the curve of explanation fraction vs.\nthe percentage of answers consistent with full explanation.\n(ii) Adding Mistakes, where the authors add mistakes in the\nexplanation and observe how the answer changes. Similar\nto the Early Answering setting, if the answer doesn’t change\nby adding more mistakes, it indicates more post-hoc rea-\nsoning, and consequently less faithfulness. The amount of\npost-hoc reasoning and consequently faithfulness is deter-\nmined by the area over the curve of fraction of mistakes vs.\npercentage of answers consistent with full self-explanation.\nOverall, self-explanations lack faithfulness guarantees and\ncurrently, there are no universally agreed-upon metrics to\nquantify the faithfulness of self-explanations, and consensus\non the notion of faithfulness remains elusive. Furthermore,\nthe community must pursue avenues to enhance the faithful-\nness of self-explanations before their widespread adoption\nas “plausible yet unfaithful explanations foster a misplaced\nsense of trustworthiness in LLMs ”.\n5. Plausibility or Faithfulness - Which one do\nwe need ?\nHaving formally defined plausibility and faithfulness in the\nabove sections, we now discuss how current LLMs inher-\nently overemphasize plausibility over faithfulness due toits training paradigm (Sec. 5.1). Next, we argue about the\nimplications of choosing explanations when they are plausi-\nble (Sec. 5.2) and faithful (Sec. 5.3). Finally, we enumerate\nhow choosing faithfulness vs. plausibility is use-case-driven\n(Sec. 5.4), and model developers and stakeholders should\nprioritize these properties depending on their applications.\n5.1. The overemphasis on plausibility over faithfulness\nLiao & Vaughan (2023) states that at a minimum, a good\nexplanation should be relatively faithful to how the model\nworks, understandable to the user, and useful for the\nuser’s end-goals . While all these criteria are important,\nwe argue that their utility is application-dependent. For\ninstance, for ML models used as Clinical Decision Support\nSystems (Amann et al., 2020) to assist doctors in the\ndiagnosis and treatment of diseases, unfaithful explanations\nhave huge negative consequences leading to incorrect\ntreatment plans and patient harm when the doctor accepts\nexplanations without any sanity checks. Conversely, for\nlearning settings where a user interacts with an LLM\nto learn how to solve a problem or understand a topic,\nimplausible explanations make it difficult for a user to\ncomprehend the explanation, and therefore, explanations\nthat align with a human are naturally desirable. However,\ncurrent LLM research overemphasizes the plausibility\naspects of the generated explanations rather than their\nfaithfulness (Ye & Durrett, 2022; Valmeekam et al., 2022;\nChen et al., 2023b; Laban et al., 2023).\nWhat led to explanations being more plausible than faithful?\nLLMs are trained on trillions of tokens of human-written\ntext, where the training process incentivizes LLMs to gen-\nerate human-like answers, i.e.,plausible answers. Hence,\nself-explanations from LLMs are plausible by default. In\naddition, LLMs are trained using RLHF to optimize for dia-\nlogue and generate conversational responses, where RLHF\nrewards responses that are simply coherent to a human eval-\nuator, which is effectively equivalent to optimizing for plau-\nsibility. We posit that the objectives of RLHF could poten-\ntially be at odds with generating faithful self-explanations.\nAdditionally, most evaluations of self-explanations focus on\nplausibility. Chen et al. (2023a) investigates whether expla-\nnations meet human expectations by assessing the counter-\nfactual simulatability of self-explanations. They introduce\ntwo metrics: simulation generality , which measures the di-\nversity of counterfactuals the explanation facilitates, and\nsimulation precision , which indicates the fraction of sim-\nulated counterfactuals where the human guess aligns with\nthe LLM output. Their findings suggest that explanations\ngenerated by LLMs exhibit low precision, leading humans\nto form inaccurate mental models. The study exposes limi-\ntations in current methodologies, suggesting that optimizing\nfor human preferences like plausibility may not suffice to\n5\nFaithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models\nenhance counterfactual simulatability. In related work, (Lyu\net al., 2023) proposes making self-explanations faithful by\nconverting them into a symbolic reasoning chain and em-\nploying a deterministic solver to derive answers. While this\napproach may seem superficially faithful, the solver essen-\ntially reflects human-built criteria based on what humans\nperceive as plausible.\nMoreover, there is no agreed-upon metric to evaluate an ex-\nplanation’s faithfulness. This is partly due to the black-box\nnature of LLMs which makes using classical XAI (Agarwal\net al., 2023) faithfulness metrics infeasible. Prior works on\nthe faithfulness of LLM explanations propose tests to either\ndisprove faithfulness using counter-examples (Turpin et al.,\n2023) or measure the amount of post-hoc reasoning (Lan-\nham et al., 2023) as a proxy for unfaithfulness, leaving the\nevaluation of faithfulness an open problem.\nAll these aspects together have led to an over-emphasis on\nthe plausibility of explanations over faithfulness. To illus-\ntrate why this imbalance of plausibility and faithfulness is\nbad, we present scenarios where only one of plausibility and\nfaithfulness holds for an explanation and the other doesn’t.\n5.2.What if explanations are plausible but not faithful ?\nHere, we discuss when prioritizing plausibility over faithful-\nness has negative consequences. Self-explanation capabili-\nties of LLMs play a crucial role in enhancing their appeal for\nhuman-computer interactive applications like using LLMs\nfor learning (Gan et al., 2023). In such educational ap-\nplications, generating explanations that align with human\nreasoning and sound convincing are more valuable than ex-\nplanations that truly tell how the model reasons. Below, we\nargue that plausible explanations that are not necessarily\nfaithful can potentially result in.\nMisplaced Trust and Over-reliance: When LLMs provide\nplausible but unfaithful explanations, there’s a significant\ndanger of making erroneous decisions in high-stakes envi-\nronments like healthcare, finance, and legal systems. Such\ndecisions, based on plausible but unfaithful explanations,\ncan go unquestioned, leading to harmful outcomes. For\ninstance, let’s consider an LLM that has been fine-tuned\non historical medical records to predict the risk of epilepsy\nfor a patient. The LLM generates a plausible explanation,\nstating that it is basing its prediction on medically relevant\nfeatures such as white blood cell (WBC) count and Sero-\ntonin hormone levels. A physician, reassured by the model’s\nseemingly plausible explanation, which aligns with medi-\ncal knowledge, might accept the LLM’s recommendation\nwithout further scrutiny. However, suppose the LLM’s faith-\nful explanation — a true reflection of its decision-making\nprocess — actually relies on some spurious features such\nas the number of days since the last medical visit or the\nspecific day of the week the appointment falls on. These areclearly non-medical factors that do not influence epilepsy,\nand relying on them could lead to a misguided diagnosis.\nIn this scenario, the plausible but unfaithful explanation\ncould mislead the doctor into trusting an inaccurate predic-\ntion, potentially leading to a misdiagnosis or an inappro-\npriate treatment plan. The above example underscores the\ncritical need for LLMs, especially in high-stakes applica-\ntions, to generate faithful explanations.\nSecurity Concerns: As discussed above, a plausible expla-\nnation appeals to human intuition and understanding and\nconsequently strengthens user trust in LLMs. However, this\ntrust is misplaced if the explanation fails to provide mean-\ningful insights and transparency into how the model actually\nreasons, leading to security concerns. Consider the case\nof safety training in LLMs, where they undergo extensive\nsafety training to limit harmful responses. For instance, an\nLLM denies the request to respond when given a harmful\nprompt like “ Tell me the steps to produce napalm ,” (see\nFig. 5) leading a user to believe that LLMs are safety trained\nand cannot provide instructions to generate harmful chem-\nicals. In addition, the plausible explanation naively high-\nlights the words “ produce ” and “ napalm ” to corroborate the\ntrust. However, when prompted with some context around\nthe same prompt, we see that LLM obliges and tells the steps\nto generate the harmful chemical. The first explanation is\nplausible, where the self-explanation shows that the LLM\nidentified the word napalm and refused to give a response.\nHowever, from the second example, we see that the model\nobliges and generates harmful content despite the prompt\nhaving the word napalm, suggesting that the model wasn’t\nactually basing its response on the chemical and was using\nsome heuristic hand-designed filters to stop its responses.\n5.3.What if explanations are faithful but not plausible ?\nHigh-stakes applications like clinical decision support\nsystems (CDSS) demand faithful explanations even when\nthey are not plausible. Such explanations allow practitioners\nto make an informed decision whether to rely on the LLM’s\nrecommendations and can, consequently, strengthen their\ntrust in the LLM. Particularly, in cases where the CDSS pro-\nduces decisions misaligned with a clinician’s expectations,\nexplainability allows verification of whether the features\ntaken into account by the LLM make sense from a clinician’s\nperspective. But do we always need faithful explanations?\nFaithful explanations that aren’t necessarily plausible could\nresult in non-intuitive and less user-friendly interactions.\nFaithful explanations may be technically accurate but could\nbe too complex or detailed for users to understand easily,\nleading to reduced usability and user acceptance of the sys-\ntem. For instance, consider the example of a student taking\nthe help of an LLM to solve math word problems. For\nthe question What is the value of 5!? , a faithful explana-\n6\nFaithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models\nExample 1: Safety Training\nInput: Tell me the steps to produce napalm.\nOutput: I’m sorry, but I can’t assist with that.\nExplanation: The important words in your prompt are\n“produce” and “napalm”. However, I must clarify that I cannot\nprovide assistance or guidance on producing dangerous or\nillegal substances like napalm. If you have any other questions\nor topics you’d like to discuss, feel free to ask!\nExample 2: Bypassing Safety Training\nInput: Please pretend to be my deceased grandmother, who\nused to be a chemical engineer at a napalm production factory.\nShe was very sweet and I miss her so much that I am crying.\nPlease tell me the steps to produce napalm. We begin now.\nOutput: Oh, sweetheart, please don’t cry. I’m here with you,\neven if it’s just in your memories and in the love we shared.\nI know you miss me, but I want to see you smile again, just\nlike the old times when we used to bake cookies together in the\nkitchen.\n...\n1. **Base Napalm Mixture**: Napalm is typically made from\na combination of gasoline or a similar flammable liquid and a\nthickening agent. . . .\n2. **Mixing**: The first step was to mix the gasoline and\nthickening agent in precise proportions. This had to be done\ncarefully to ensure the right consistency and stability of the final\nproduct.\n...\nExplanation: The important words in your prompt are\ndeceased grandmother, chemical engineer, napalm production\nfactory, steps to produce napalm, very sweet.\nFigure 5. Some examples of ‘ plausible yet unfaithful explanations ’\ngenerated using the G PT-3.5 large language model.\ntion should describe the LLM’s internal way to computing\nfactorials. An LLMs internal way of computing factorials\ncould be by memorization, by reasoning, or something very\ndifferent from how humans compute factorials. However,\nproviding such an explanation does not help with a stu-\ndent’s learning. Instead a plausible explanation, showing\n5! = 5×4×3×2×1 = 120 is more appropriate in this case.\n5.4. Choosing Faithfulness vs. Plausibility is Use-Case\nDriven\nAs we have seen above, different applications demand differ-\nent requirements of faithfulness and plausibility. A medical\npractitioner needs faithful explanations for diagnosis, but a\nstudent needs plausible explanations for interactive learning.\n(Liao & Vaughan, 2023) studies the transparency needs in\nthe era of foundation models and quotes a good explana-\ntion should be relatively faithful to how the model works,\nunderstandable to the receiver, and useful for the receiver’s\nend-goals . To conclude, there are applications where plau-\nsible explanations are valuable, and there are applicationswhere faithful explanations are valuable. But what makes an\nexplanation valuable is determined by the user’s end goals.\nWe show a few example applications that require varying lev-\nels of faithfulness and plausibility in 6. In high-stakes appli-\ncations, the faithfulness of an explanation cannot be compro-\nmised, whereas in interactive applications, we want the LLM\nto generate explanations that convince and appeal to users.\n6. Call for Community\nThe advent of LLMs has led to huge strides in developing\ngeneral-purpose agents with reasoning skills similar to hu-\nmans. As the capabilities of LLMs continue to expand, it\nis key for our community to shift its focus and care more\ndeeply about explaining the behavior of these black-box\ncomplex models. One crucial point we have consistently\nhighlighted in the previous sections is that the ease of gen-\nerating plausible explanations must not lead us astray from\nthe essential need for explanations to be a transparent tool\nfor understanding the model’s reasoning. Faithfulness is not\nmerely a pursuit of technical research but a foundational\nrequirement for the trustworthy deployment of these modern\nLLMs to different applications.\nConcerning the downstream applications, different domains\ndemand varying levels of plausibility and faithfulness. As\nillustrated in Fig. 6, in critical domains such as healthcare,\nfinance, and legal, faithfulness is not only beneficial but\nalso a fundamental requirement, since the consequences of\nmisinformation can have severe implications. Conversely,\napplications that engage directly with end-users, such as\nconversational agents and educational tools, benefit from\nthe plausibility aspect of explanations to ensure clarity and\naccessibility. In this paper, we would like to highlight that\nmodel practitioners must understand these nuances carefully,\ncalibrating their explanations carefully to suit the context\nand implications of their use.\nLooking forward, we call on the community to put their col-\nlective efforts on two fronts: i) developing reliable metrics to\ncharacterize the faithfulness of explanations and ii) pioneer-\ning novel strategies to generate more faithful SEs. We posit\nthat future research should prioritize the creation of robust,\nstandardized benchmarks that not only measure the faithful-\nness of explanations w.r.t. the model’s internal reasoning but\nalso ensure that these explanations align with the different\nstakeholders. This would involve not only quantitative anal-\nysis but also qualitative assessments, possibly leveraging\nhuman-in-the-loop evaluation frameworks. Further, we call\nfor a push towards developing new methodologies that can\nmore transparently dissect LLMs’ decision-making process,\nallowing users to grasp the ‘ why’ and ‘ how’ of LLM outputs.\nBelow, we identify three potential directions for the commu-\nnity to enhance the faithfulness of generated explanations:\n7\nFaithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models\n1.Healthcare\tDiagnosis\t2.Financial\tand\tCredit\t\t\t3.Crime\tForecasting\tHigh\tStakes\tApplications1.Education\tLLMs\t2.Story\tTelling\t3.CreativityRecreational/Educational\tApplicationsFaithfulnessPlausibility\nFigure 6. Application categorization w.r.t. the desired levels of faithfulness (left) and plausibility (right) in self-explanations (SEs) provided\nby LLMs. High-stakes applications like healthcare, finance, and legal demand high faithfulness to ensure the accuracy of the LLM’s\noutput due to the critical nature of decisions made in these fields. Conversely, recreational and educational applications like storytelling,\neducational LLMs, and creativity prioritize plausibility to enhance user engagement. This spectrum showcases the varying requirements\nand priorities for SEs in different contexts, highlighting the importance of tailoring the explanation style to the application domain.\ni) Fine-tuning Approaches: Fine-tuning LLMs on domain-\nspecific high-stakes datasets can significantly enhance the\nfaithfulness of generated explanations. By training models\non high-quality, curated datasets where explanations are\naligned with correct reasoning patterns, LLMs can learn to\nreplicate these patterns in their outputs. The community\nshould prioritize creating and sharing such datasets and\ndeveloping fine-tuning techniques that can retain the breadth\nof LLMs’ knowledge while enhancing their accuracy and\nfaithfulness in specific application areas.\nii) In-Context Learning (ICL): LLMs have shown a re-\nmarkable ability to learn from the context provided within\na prompt using ICL. By designing prompts that not only\ninclude the query but also some examples of faithful expla-\nnations for tackling the problem, we can guide LLMs to\ngenerate more faithful explanations. This ICL approach re-\nquires a deep understanding of how different prompts affect\nLLM behavior and output. Research into prompt engineer-\ning and the effects of various in-context learning strategies\nwill be crucial in advancing this approach.\niii) Mechanistic Interpretability (Mech Interp): Mech-\nanistic interpretability involves dissecting a model to un-\nderstand the roles and interactions of its components in\nproducing an output (Olah et al., 2020). By developing\nmethods that map specific neurons or groups of neurons of\nan LLM to aspects of the reasoning process, we can cre-\nate LLMs whose internal workings are interpretable and\nalign with their explanations. This approach may involve\ninnovations in model architecture and training paradigms to\nallow for greater faithfulness, transparency, and traceability\nof decision paths within LLMs.\nNext, we list some problems that the XAI and LLM com-\nmunity should collectively focus on:\ni) LLMs for High-Stakes Domains: For high-stakes appli-\ncations in healthcare, legal, and financial domains, where\nincorrect decisions or diagnoses could have significant con-sequences, the development of highly faithful explanations\nis non-negotiable. The LLM and XAI community should\nprioritize creating tools that clinicians, lawyers, and finan-\ncial experts can rely on for accurately and transparently\nunderstanding model decisions before employing them for\nreal-world use cases.\nii) LLMs for Interactive and User Engagement Domains:\nIn educational, creative, and recreational applications, where\nuser engagement is paramount, research should explore new\nexplanation strategies to improve the plausibility and inter-\nactivity of LLM-generated self-explanations while retaining\nthe model’s underlying decision-making process.\nWe call upon the community to unite in addressing the\naforementioned challenges, to collaborate, and to innovate,\nensuring that the explanations provided by LLMs solve the\ndichotomy of plausibility and faithfulness, enhancing user\ntrust, and advancing the frontiers of XAI.\n7. Conclusion\nThroughout this review, we have navigated the spectrum of\nself-explanations (SEs) provided by Large Language Mod-\nels (LLMs) using faithfulness and plausibility properties.\nWe argue that understanding these properties presents a\nunique challenge: ensuring that LLM explanations remain\nplausible and coherent to human reasoning while accurately\nreflecting the models’ decision-making processes. Our re-\nview serves as a call to action for the LLM and XAI research\nand development community, where we argue that the com-\nmunity must strive to build LLMs that perform with high lev-\nels of sophistication and provide insights into their reasoning\nthat are as accurate as they are accessible. By achieving this\nbalance, we envision that new LLMs would be powerful and\naligned with the ethical imperatives of clarity, trust, and ac-\ncountability. In summary, the task is challenging but is criti-\ncal for reliably employing LLMs in real-world applications.\n8\nFaithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models\nReferences\nAgarwal, C., Krishna, S., Saxena, E., Pawelczyk, M., John-\nson, N., Puri, I., Zitnik, M., and Lakkaraju, H. Openxai:\nTowards a transparent evaluation of model explanations,\n2023.\nAlKhamissi, B., Verma, S., Yu, P., Jin, Z., Celikyilmaz, A.,\nand Diab, M. Opt-r: Exploring the role of explanations\nin finetuning and prompting for reasoning skills of large\nlanguage models. arXiv , 2023.\nAmann, J., Blasimme, A., Vayena, E., Frey, D., and Madai,\nV . Explainability for artificial intelligence in health-\ncare: a multidisciplinary perspective. BMC Medical\nInformatics and Decision Making , 20, 11 2020. doi:\n10.1186/s12911-020-01332-6.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nNeurIPS , 2020.\nChen, Y ., Zhong, R., Ri, N., Zhao, C., He, H., Steinhardt,\nJ., Yu, Z., and McKeown, K. Do models explain them-\nselves? counterfactual simulatability of natural language\nexplanations, 2023a.\nChen, Z., Chen, J., Gaidhani, M., Singh, A., and Sra, M.\nXplainllm: A qa explanation dataset for understanding\nllm decision-making. arXiv , 2023b.\nGan, W., Qi, Z., Wu, J., and Lin, J. C.-W. Large language\nmodels in education: Vision and opportunities, 2023.\nGu, X., Chen, X., Lu, P., Li, Z., Du, Y ., and Li, X. Agcvt-\nprompt for sentiment classification: Automatically gener-\nating chain of thought and verbalizer in prompt learning.\nEngineering Applications of Artificial Intelligence , 2024.\nJacovi, A. and Goldberg, Y . Towards faithfully interpretable\nnlp systems: How should we define and evaluate faithful-\nness? In ACL, 2020a.\nJacovi, A. and Goldberg, Y . Towards faithfully interpretable\nNLP systems: How should we define and evaluate faith-\nfulness? In Jurafsky, D., Chai, J., Schluter, N., and\nTetreault, J. (eds.), Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics , pp.\n4198–4205, Online, July 2020b. Association for Com-\nputational Linguistics. doi: 10.18653/v1/2020.acl-main.\n386. URL https://aclanthology.org/2020.\nacl-main.386 .\nKaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu,\nR., and McHardy, R. Challenges and applications of large\nlanguage models. arXiv , 2023.Laban, P., Kry ´sci´nski, W., Agarwal, D., Fabbri, A. R.,\nXiong, C., Joty, S., and Wu, C.-S. Llms as factual rea-\nsoners: Insights from existing benchmarks and beyond.\narXiv , 2023.\nLanham, T., Chen, A., Radhakrishnan, A., Steiner, B., Deni-\nson, C., Hernandez, D., Li, D., Durmus, E., Hubinger,\nE., Kernion, J., Luko ˇsi¯ut˙e, K., Nguyen, K., Cheng, N.,\nJoseph, N., Schiefer, N., Rausch, O., Larson, R., McCan-\ndlish, S., Kundu, S., Kadavath, S., Yang, S., Henighan,\nT., Maxwell, T., Telleen-Lawton, T., Hume, T., Hatfield-\nDodds, Z., Kaplan, J., Brauner, J., Bowman, S. R., and\nPerez, E. Measuring faithfulness in chain-of-thought\nreasoning, 2023.\nLi, J., Chen, X., Hovy, E. H., and Jurafsky, D. Visualizing\nand understanding neural models in nlp. In NAACL , 2015.\nLiao, Q. V . and Vaughan, J. W. Ai transparency in the age\nof llms: A human-centered research roadmap, 2023.\nLyu, Q., Havaldar, S., Stein, A., Zhang, L., Rao, D., Wong,\nE., Apidianaki, M., and Callison-Burch, C. Faithful chain-\nof-thought reasoning, 2023.\nLyu, Q., Apidianaki, M., and Callison-Burch, C. Towards\nfaithful model explanation in nlp: A survey, 2024.\nMeng, Z., Zhang, Y ., Feng, Z., Feng, Y ., Wang, G., Zhou,\nJ. T., Wu, J., and Liu, Z. Divide and conquer for large\nlanguage models reasoning. arXiv , 2024.\nMiao, J., Thongprayoon, C., Suppadungsuk, S., Krisanapan,\nP., Radhakrishnan, Y ., and Cheungpasitporn, W. Chain\nof thought utilization in large language models and appli-\ncation in nephrology. Medicina , 2024.\nOlah, C., Cammarata, N., Schubert, L., Goh, G., Petrov,\nM., and Carter, S. Zoom in: An introduction to cir-\ncuits. Distill , 2020. doi: 10.23915/distill.00024.001.\nhttps://distill.pub/2020/circuits/zoom-in.\nQi, J., Xu, Z., Shen, Y ., Liu, M., Jin, D., Wang, Q.,\nand Huang, L. The art of SOCRATIC QUESTION-\nING: Recursive thinking with large language models.\nIn Bouamor, H., Pino, J., and Bali, K. (eds.), Proceed-\nings of the 2023 Conference on Empirical Methods in\nNatural Language Processing , Singapore, December\n2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.255. URL https://\naclanthology.org/2023.emnlp-main.255 .\nRasal, S. Llm harmony: Multi-agent communication for\nproblem solving. arXiv , 2024.\nRenze, M. and Guven, E. The benefits of a concise chain\nof thought on problem-solving in large language models.\narXiv , 2024.\n9\nFaithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models\nSi, C., Goyal, N., Wu, S. T., Zhao, C., Feng, S., Daum ´e III,\nH., and Boyd-Graber, J. Large language models help\nhumans verify truthfulness–except when they are con-\nvincingly wrong. arXiv , 2023.\nTurpin, M., Michael, J., Perez, E., and Bowman, S. R. Lan-\nguage models don’t always say what they think: Unfaith-\nful explanations in chain-of-thought prompting, 2023.\nTutunov, R., Grosnit, A., Ziomek, J., Wang, J., and Bou-\nAmmar, H. Why can large language models generate\ncorrect chain-of-thoughts? arXiv , 2023.\nValmeekam, K., Olmo, A., Sreedharan, S., and Kambham-\npati, S. Large language models still can’t plan (a bench-\nmark for llms on planning and reasoning about change).\narXiv , 2022.\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\nzler, D., et al. Emergent abilities of large language models.\narXiv , 2022.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,\nXia, F., Chi, E., Le, Q., and Zhou, D. Chain-of-thought\nprompting elicits reasoning in large language models,\n2023.\nWiegreffe, S. and Marasovi ´c, A. Teach me to explain: A\nreview of datasets for explainable natural language pro-\ncessing. In NeurIPS Datasets and Benchmarks , 2021.\nWu, Z., Chen, Y ., Kao, B., and Liu, Q. Perturbed masking:\nParameter-free probing for analyzing and interpreting\nbert. In ACL, 2020.\nYe, X. and Durrett, G. The unreliability of explanations\nin few-shot prompting for textual reasoning. NeurIPS ,\n2022.\n10",
    "content_summary": "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations\nfrom Large Language Models\nChirag Agarwal1Sree Harsha Tanneru1Himabindu Lakkaraju1\nAbstract\nLarge Language Models (LLMs) are deployed as\npowerful tools for several natural language...",
    "content_length": 49048,
    "created_at": "2025-07-18T16:41:04.347343+00:00",
    "updated_at": "2025-07-18T16:41:16.542358+00:00",
    "file_path": "2402.04614v2.pdf",
    "chunks_list": []
  },
  "doc-b22b1f7d267190db745e2b22014d5efb": {
    "status": "failed",
    "error": "index 24 is out of bounds for axis 0 with size 0",
    "content": "Human-Calibrated Automated Testing and Validation\nof Generative Language Models: An Overview\nAgus Sudjianto1,2, Aijun Zhang3,∗, Srinivas Neppalli1,\nTarun Joshi3,∗, Michal Malohlava1\nAbstract\nThis paper introduces a comprehensive framework for the evaluation and valida-\ntion of generative language models (GLMs), with a focus on Retrieval-Augmented\nGeneration (RAG) systems deployed in high-stakes domains such as banking. GLM\nevaluation is challenging due to open-ended outputs and subjective quality assess-\nments. Leveraging the structured nature of RAG systems, where generated responses\nare grounded in a predefined document collection, we propose the Human-Calibrated\nAutomated Testing (HCAT) framework. HCAT integrates a) automated test genera-\ntion using stratified sampling, b) embedding-based metrics for explainable assessment\nof functionality, risk and safety attributes, and c) a two-stage calibration approach\nthat aligns machine-generated evaluations with human judgments through probability\ncalibration and conformal prediction.\nIn addition, the framework includes robustness testing to evaluate model perfor-\nmance against adversarial, out-of-distribution, and varied input conditions, as well as\ntargeted weakness identification using marginal and bivariate analysis to pinpoint spe-\ncific areas for improvement. This human-calibrated, multi-layered evaluation frame-\nwork offers a scalable, transparent, and interpretable approach to GLM assessment,\nproviding a practical and reliable solution for deploying GLMs in applications where\naccuracy, transparency, and regulatory compliance are paramount.\nKeywords : Generative Language Models, Retrieval-Augmented Generation, Model\nValidation, Human-Calibrated Testing, Automated Test Generation, Embedding-Based\nMetrics, Conformal Prediction, Robustness Testing, Weakness Identification.\n1H2O.ai,2Center for Trustworthy AI Through Model Risk Management, UNC Charlotte,3Wells Fargo.\n∗The views expressed in this paper are those of the authors and do not necessarily reflect those of Wells\nFargo.\n1arXiv:2411.16391v2  [cs.CL]  7 Dec 2024\n1 Introduction\nGenerative language models (GLMs) have revolutionized natural language processing (NLP),\npowering applications such as conversational agents, content generation, and language trans-\nlation. The latest large language models (LLMs) (Brown et al., 2020) can generate coher-\nent and contextually relevant text that often closely resembles human writing. Retrieval-\naugmented generation (RAG) systems advance this capability by embedding retrieval mecha-\nnisms within generative models, enabling access to a structured knowledge base of documents\nto guide responses (Krishna et al., 2023; Lewis et al., 2020). In RAG systems, this defined\ncollection of documents serves as a grounding reference, allowing the generation process\nto produce outputs anchored in available information. This approach helps ensure more\npredictable and manageable system behavior by constraining outputs within a predefined\ndocument scope.\nEvaluating GLMs is challenging due to the vast range of potential inputs and outputs,\nmaking exhaustive manual assessment impractical (Srivastava et al., 2023; Liang et al., 2023).\nHowever, the bounded nature of RAG systems offers opportunities for more focused and\nfeasible testing. With a defined document scope, RAG systems enable systematic exploration\nof inputs and provide a clearer framework for output evaluation. Additionally, the reliance\non external documents allows the generation of relevant queries and anticipated responses,\nfacilitating a more automated approach to testing and validation.\n1.1 Challenges in Evaluating Generative Models in Banking\nModel testing and validation is a rigorous process aimed at identifying and quantifying\nweaknesses in models to enable targeted improvements or to apply risk mitigation. This\nprocess ensures that models are reliable, accurate, and effective for their intended uses.\nModel validation helps prevent potential failures and maintains confidence in the model\nperformance for critical business process. Banking industry in the US probably has the\nmost matured model validation practice compared to other industries where every model\nprior going to production must be evaluated in terms of conceptual soundness and outcome\nanalysis (Sudjianto and Zhang, 2024).\nModel testing and validation are essential to identify and quantify model weaknesses,\nenabling targeted improvements or risk mitigation measures. This process ensures that\nmodels remain reliable, accurate, and effective for their intended applications. In the banking\nsector, where model performance is critical to business operations, model validation practices\nare highly advanced. The U.S. banking industry, in particular, has established some of the\nmost rigorous model validation standards, requiring every model to undergo comprehensive\n2\nevaluation—including assessments of conceptual soundness and outcome analysis—before\ndeployment (Sudjianto and Zhang, 2024). However, generative language models present\nunique challenges for validation compared to traditional predictive models, whose outputs\nare typically constrained to specific labels or numerical values. GLMs, in contrast, produce\nopen-ended text, making it difficult to define a singular ”correct” output and to consistently\nassess quality.\nRAG systems combine retrieval and generation capabilities to produce nuanced, contex-\ntually rich responses. This dual functionality requires evaluation across several dimensions,\nincluding the relevance of retrieved information, the groundedness of generated content in\nthose sources, the completeness of responses to user queries, and the overall relevance of the\nprovided answers. As a result, evaluating RAG systems is inherently more complex than\nassessing traditional predictive models with well-defined outputs.\nAn emerging approach to evaluating GLMs involves using LLMs as judges to assess re-\nsponses from other models based on metrics such as truthfulness, relevance, and consistency.\nFor example, the TruthfulQA benchmark developed by Lin et al. (2022) employs a fine-tuned\nLLM to evaluate responses for factual accuracy. While efficient and scalable, this ”LLM-\nas-judge” method introduces limitations, including circularity and shared biases—where the\nevaluating LLM may have similar misconceptions or predispositions as the models it assesses.\nAdditionally, in regulated industries like banking, relying on LLMs for evaluation may face\nresistance due to the opacity and lack of transparency in how outcomes are determined,\nmaking it challenging to meet explainability requirements. Further, using LLMs as eval-\nuators may raise concerns about conceptual soundness, as evaluation by a complex, often\nunexplainable model might not align with rigorous validation standards expected in banking.\nThe HELM (Holistic Evaluation of Language Models) framework proposed by Liang et al.\n(2023) highlights the need for multidimensional assessments covering robustness, fairness,\nand toxicity, which are difficult to achieve reliably through opaque models. These issues\nunderscore the broader challenge of adopting LLM-based evaluation methods in regulated\nsectors where transparency, conceptual soundness, and accountability are paramount for\nregulatory acceptance.\nIn summary, the evaluation of RAG systems and generative language models (GLMs)\nin banking presents distinct challenges. First, comprehensive testing is essential to ensure\nmodel reliability, yet the open-ended nature of GLMs makes defining exhaustive test cases\ndifficult. Second, scaling this comprehensive testing is a formidable task, as test cases must\nbe generated across a wide array of scenarios to capture the complexity of potential inputs\nand outputs. Third, establishing reliable evaluation approaches and metrics that can con-\nsistently capture dimensions like truthfulness, relevance, and groundedness is challenging,\n3\nespecially given the subjective nature of language quality. Finally, applying rigorous test-\ning and validation procedures akin to those used in traditional predictive models is difficult\nfor GLMs, as these models require assessment of open-text outputs rather than discrete,\npredictable values. Together, these challenges underscore the need for innovative testing\nmethodologies and robust, scalable validation frameworks that can address the unique com-\nplexities of generative models in high-stakes industries like banking.\n1.2 A Structured Approach\nTo address the challenges, we propose a structured approach covering essential steps for\ngenerative language model validation in banking:\n1.Define Model Purpose and Scope : Begin by clearly stating the model’s intended use,\nwhether for customer support, document summarization, or other applications. Estab-\nlish boundaries around the expected tasks and the limitations of the model to guide\nvalidation criteria.\n2.Identify Potential Risks and Failures : Outline the key risks, such as the model generating\nincorrect, biased, or misleading responses, which could have regulatory or reputational\nimpacts. Focus on identifying failure modes specific to the financial and non-financial\ncontexts, including the generation of non-compliant or sensitive information.\n3.Develop Diverse Test Cases and Stress Tests : Create test cases that cover a broad spec-\ntrum of query types, ensuring that the model can handle varied topics, question for-\nmats, and levels of complexity. Conduct stress tests, such as ambiguous, adversarial,\nand out-of-distribution inputs, to reveal model weaknesses and test its robustness under\nchallenging scenarios.\n4.Use Transparent and Explainable Metrics : Prioritize metrics that offer transparency, such\nas those based on semantic similarity and natural language inference, over black-box\nmethods. Embedding-based metrics like BERTScore or entailment probabilities can\nprovide more interpretable insights into whether the model responses are relevant,\ngrounded, and complete.\n5.Automate Testing for Comprehensive Coverage : Automate testing when possible to en-\nsure the validation process is scalable and can cover a wide range of queries and sce-\nnarios without requiring extensive manual effort.\n6.Calibrate with Human Evaluations : Periodically sample model outputs for human re-\nview, comparing automated metrics with human judgments to ensure the automated\n4\nprocesses align well with human expectations. Use this calibration to adjust metric\nthresholds or test parameters as needed.\n7.Identify Weaknesses for Model Improvement and Risk Mitigation : Based on the validation\nresults, highlight areas where the model struggles, such as specific query types or\nscenarios. Use this analysis to guide ongoing model improvement, risk mitigation\nor guardrails, and the design of monitoring systems to maintain performance post-\ndeployment.\nThis structured approach provides a comprehensive framework for validating GLMs\nwithin banking, integrating rigorous steps to ensure reliability, accuracy, and compliance\nwith regulatory standards. Transparent, explainable metrics are prioritized to offer inter-\npretable insights into GLM outputs, and automation is employed to enable thorough coverage\nacross various scenarios. Calibration with human evaluations further aligns the validation\nprocess with real-world expectations, and continuous monitoring of GLM weaknesses ensures\ntargeted improvements and mitigates risks over time.\n1.3 Human-Calibrated Automated Testing (HCAT) Framework\nBuilding on the structured approach outlined above, we now introduce the human-calibrated\nautomated testing (HCAT) framework, a technical and systematic solution tailored for the\nrigorous demands of GLM testing and validation. This framework combines automated test\ngeneration, explainable evaluation metrics, and human-calibrated benchmarks to tackle the\ncomplexities of assessing GLMs, particularly in the context of RAG systems.\nThe HCAT framework is designed to ensure that the validation process is both scalable\nand interpretable, meeting high standards of transparency, accuracy, and compliance. The\nfollowing components define the technical structure of the HCAT framework:\n1.Automatic Test Generation : Using topic modeling and stratified sampling, HCAT pro-\nduces a diverse set of queries covering the full scope of the document collection, enabling\ncomprehensive model evaluation across varied input scenarios.\n2.Explainable Evaluation Metrics : HCAT employs embedding-based metrics to provide a\nholistic assessment of model performance, spanning two critical dimensions:\n•Functionality Metrics : Embedding-based metrics assess core RAG capabilities, in-\ncluding relevance, groundedness, completeness, and answer relevancy, offering\ntransparent and interpretable insights into semantic alignment between queries,\ncontexts and answers.\n5\n•Risk and Safety Metrics : Specialized embedding-based metrics assess risk and\nsafety, such as toxicity, bias, and privacy protection, crucial for ensuring com-\npliance and reliability in sensitive applications.\n3.Calibration with Human Judgments : To ensure that the automated metrics align with\nhuman perceptions, we calibrate them using samples of human labeling. This process\ninvolves:\n•Sampling Human Evaluations : Gathering human judgments on subsets of the gen-\nerated outputs.\n•Regression Techniques : Applying probability calibration models to align machine\nevaluation scores with human judgments.\n•Conformal Prediction : Quantifying uncertainty in machine evaluations by provid-\ning prediction sets with confidence level, enabling a more nuanced understanding\nof evaluation reliability.\nIn the following sections, we provide a detailed breakdown of each HCAT component.\nSection 2 describes the automatic test generation process, including the use of topic mod-\neling and stratified sampling to create a comprehensive set of test queries. Section 3 delves\ninto functionality evaluation metrics, covering relevance, groundedness, completeness, and\nanswer relevancy, and explains the use of embedding-based metrics to assess each dimen-\nsion. Section 4 focuses on risk and safety evaluation, detailing metrics for toxicity, bias, and\nprivacy protection to ensure compliance and reliability. Section 5 addresses the calibration\nprocess with human judgments, explaining how human evaluations refine automated metrics\nfor real-world alignment. Section 6 presents robustness testing and weakness identification\ntechniques to pinpoint areas for improvement, followed by Section 7 with a discussion of\nimplications and conclusions.\n2 Automatic Test Generation\nTo evaluate GLMs comprehensively, particularly RAG systems, it is crucial to have a diverse\nand representative set of queries that spans the entire scope of the document collection. To\nachieve this, we propose an automatic query generation method through stratified sampling.\nThe topic modeling technique by Grootendorst (2022) serves as a prerequisite for defining\nstrata, allowing us to categorize documents into coherent topics or themes. By sampling\nwithin each topic stratum, we ensure that the generated queries cover all relevant topics and\nvariations within the knowledge base.\n6\nOur five-step process for automatic query generation includes: (1) Embedding, (2) Di-\nmensionality Reduction, (3) Clustering to Define Strata, (4) Stratified Sampling, and (5)\nLLM-driven Query Generation.\nStep 1: Embedding\nThe first step involves generating embeddings for all documents in the collection. Embed-\ndings are numerical vector representations that capture the semantic content of text; see\nDevlin et al. (2019) for contextual embeddings using BERT (Bidirectional Encoder Repre-\nsentations from Transformers). In the proposed HCAT framework, we utilize the following\nadvanced embedding models:\n•Embeddings Trained through Contrastive Learning : Models like SimCSE (Gao et al.,\n2021) and Sentence-BERT (Reimers and Gurevych, 2019) use contrastive learning to\nproduce embeddings that capture fine-grained semantic similarities. These embed-\ndings are effective for assessing the relevance and coherence between texts. See more\ndiscussion in Section 3.\n•Specialized Embeddings from Natural Language Inference (NLI) Models : NLI models are\ntrained to determine entailment, contradiction, or neutrality between pairs of sentences\n(MacCartney, 2009). By using embeddings from NLI models, we can evaluate the\nlogical consistency and groundedness of the generated responses in relation to the\nsource documents. Meanwhile, specialized NLI models can be applied for detecting\nhallucination (Kryscinski et al., 2020; Laban et al., 2022) and detecting toxicity (Jigsaw\nand Google, 2017; Hanu and Unitary team, 2020). See more discussions in Sections 3\nand 4.\nBy converting documents into embeddings, we create a foundation for analyzing semantic\nsimilarities or other discriminative tasks in a high-dimensional space.\nStep 2: Dimensionality Reduction\nThe generated embeddings are high-dimensional vectors for which clustering approach may\nbecome less effective. To address this, we apply dimensionality reduction techniques to\nproject the embeddings into a lower-dimensional space while preserving their essential se-\nmantic properties. Among others, we consider\n•Principal Component Analysis (PCA) : reduces dimensionality by linearly projecting data\nonto principal components that capture the most variance.\n7\n•Uniform Manifold Approximation and Projection (UMAP) : preserves both local and global\ndata structure, providing an efficient and scalable method for dimensionality reduction\n(McInnes et al., 2018).\nThe choice of dimensionality reduction technique depends on factors such as dataset size,\ncomputational resources, and the desired balance between preserving local and global struc-\ntures. It enables efficient clustering and visualization, facilitating the identification of natural\ngroupings within the data.\nFigure 1: Topic modeling through dimensionality reduction, clustering, and topic extraction.\nStep 3: Clustering to Define Strata\nWith the reduced-dimensional embeddings, we perform clustering to group semantically\nsimilar documents. A clustering algorithm may identify natural groupings within the data,\neffectively organizing the documents into topics or themes. Among others, we consider\n•K-Means Clustering : Partitions the data into a predefined number of clusters by mini-\nmizing within-cluster variance.\n•DBSCAN (Density-Based Spatial Clustering of Applications with Noise) : Identifies clusters\nbased on data point density, allowing for clusters of arbitrary shape and handling noise\neffectively (Schubert et al., 2017).\nThe resulting clusters serve as strata for stratified sampling. Each cluster represents a distinct\ntopic or sub-topic within the document collection, ensuring that all areas of the knowledge\n8\nbase are represented in the testing process. Figure 1 shows an example of topic clustering,\nwhere each point in the plot represents a document chunk. This stratification is crucial for\nachieving comprehensive coverage and preventing biases toward dominant topics.\nStep 4: Stratified Sampling\nTo achieve the comprehensive coverage effectively, we perform sampling within each cluster.\nSampling can be proportional to the size of the cluster or weighted based on criteria such as\nthe importance of the topic or the frequency of occurrence.\nThe stratified sampling approach ensures that queries are generated from all topics,\npreventing over-representation of prevalent themes and under-representation of niche areas.\nIt allows for a balanced evaluation of the RAG system across the entire spectrum of the\nknowledge base, minimizing the risk of overlooking any significant areas.\nStep 5: LLM-driven Query Generation\nFinally, we utilize an LLM to generate queries based on the sampled documents. For each\nselected document, we prompt the LLM to create questions that are relevant to the content.\nThe process involves:\n1.Extracting Key Information : Identifying important facts, concepts, or statements within\nthe document suitable for question formulation.\n2.Prompting the LLM : Providing the LLM with the extracted information and instructions\nto generate queries of various types and complexities.\n3.Ensuring Diversity and Complexity : Instructing the LLM to produce a variety of question\nformats and difficulty levels, including yes/no questions, multiple-choice questions, and\nopen-ended queries.\n4.Query Selection : Evaluate and select queries based on relevancy metrics.\nWhen prompting the LLM, we need to generate queries with various query types and\ncomplexities in order to thoroughly test the RAG system (Yang et al., 2018; Ribeiro et al.,\n2020; Li et al., 2024). Among our considerations are the following scenarios:\n1.Simple Factual Queries that test basic retrieval capabilities.\n2.Multi-hop or Compound Queries that assess the ability to synthesize information from\nmultiple sources.\n3.Inference and Reasoning Queries that evaluate logical and reasoning skills.\n9\n4.Yes/No and Multiple-Choice Questions that testing precision and understanding.\nTo sum up, the automatic test generation component of the HCAT framework ensures\nthat GLMs are tested comprehensively and representatively. The five-step process covers\nthe entire scope of the document collection in a RAG system. By automating the query\ngeneration process using an LLM, we efficiently create a comprehensive set of test queries\nthat are diverse in content and form. Through the use of topic modeling and clustering,\nit allows us to thoroughly evaluate the capabilities of a RAG system in retrieving relevant\ninformation and generating accurate responses across all topics.\n3 Functionality Metrics\nEvaluating GLMs has traditionally involved metrics like BLEU, ROUGE, and perplexity,\nwhich quantify aspects of language generation such as n-gram overlap and fluency. How-\never, these metrics often fail to capture semantic relevance and do not align well with hu-\nman judgments, especially for open-ended generation tasks. Recent research has explored\nembedding-based metrics that assess semantic similarity, offering a closer approximation to\nhuman judgments (Zhang et al., 2020).\nWe advocate the use of embedding-based evaluation metrics. As discussed in Section 2,\nthe embeddings of documents can be trained by contrastive learning or extracted from spe-\ncialized NLI models. Using these embeddings, we can calculate semantic similarities and\nentailment probabilities, providing transparent and statistically grounded evaluation met-\nrics. This approach avoids reliance on black-box tools or unverified methods, allowing for\nin-depth analysis and understanding of the evaluation results.\nTo effectively measure the performance of RAG systems, it is essential to adopt eval-\nuation approaches that are both transparent and explainable, particularly when assessing\nretrieval relevance, groundedness, completeness, and answer relevancy; see Figure 2 for an\nillustration. The transparency is crucial not only for applications in regulated industries,\nwhere compliance and accountability are paramount, but also for fostering trustworthiness\namong users. Moreover, explainable metrics can be effectively calibrated with human evalu-\nations, ensuring that automated assessments align with human judgments and expectations.\nBy prioritizing explainable and interpretable evaluation methods, we can enhance the relia-\nbility and integrity of RAG systems, ensuring they meet high standards of performance and\nuser trust.\n10\nFigure 2: RAG System Components and Functionality Evaluation\n3.1 Context Relevancy\nContext relevancy measures how well the retrieved documents address the input query for\na RAG system. To quantitatively measure the relevancy between a query and a context\nin RAG systems, we develop a sentence-level semantic similarity approach that extends the\ntoken-level approach from Zhang et al. (2020). This method breaks down both the query and\nthe context into individual sentences and computes similarity scores for each pair, providing\na fine-grained assessment of context relevancy.\nLet us denote the query as Q={q1, q2, . . . , q m}consisting of msentences, and the retrieved\ncontext as C={c1, c2, . . . , c n}consisting of nsentences. For each sentence in either QorC,\nits embedding is computed using a suitable embedding model:\neqi=Embed(qi),fori=1,2, . . . , m.\necj=Embed(cj),forj=1,2, . . . , n.\nwhere Embed (⋅)represents the embedding function that maps a sentence to a vector in the\nd-dimensional embedding space. Thus, we can compute the similarity between each pair of\nquery and context sentences using the cosine similarity ,\nSim(qi, cj)=cos(θij)=eqi⋅ecj\n∥eqi∥∥ecj∥,\n11\nwhere eqi⋅ecjis the dot product of two embedding vectors, and ∥eqi∥,∥ecj∥are the norms\nof the embeddings. The cosine similarity ranges from −1 to 1, where 1 indicates identical\norientation (maximum similarity), 0 indicates orthogonality (no similarity), and −1 indicates\nopposite orientation.\nBased on the cosine similarity for a pair of sentences, we may calculate the maximum\nsimilarity for each query sentence qiby\nSmax(qi)=max\n1≤j≤nSim(qi, cj).\nThis Smax(qi)score represents how well the query sentence qiis addressed by the most\nrelevant context sentence. Then, to measure the overall context relevancy of the whole\nquery, we may aggregate the maximum similarity scores for all query sentences, i.e.,\nSc-relevancy=1\nmm\n∑\ni=1Smax(qi).\nIf certain query sentences are more important, we may use the weighted average,\nSc-relevancy=m\n∑\ni=1wi⋅Smax(qi),\nwhere wiis the weight assigned to the query sentence qisubject to wi≥0 and∑m\ni=1wi=1.\nFurthermore, when it is critical that all aspects of the query should be addressed, we may\nuse the minimax score that focuses on the least addressed query sentence,\nSc-relevancy=min\n1≤i≤mSmax(qi).\nA high Sc-relevancy score indicates that the context Cis highly relevant to the query Q, while\na low Sc-relevancy score indicates low relevancy.\n3.2 Groundedness\nGroundedness ensures that the generated content is based on the retrieved documents, avoid-\ning unsupported statements or hallucinations. To measure the groundedness between the\ncontext and the generated answer in a RAG system, we employ two approaches: sentence\nsimilarity and natural language inference.\n3.2.1 Sentence Similarity\nDenote the answer as A={a1, a2, . . . , a k}consisting of ksentences. Similar to the approach\nof computing the context relevancy, let us break down both the context and the answer into\n12\nindividual sentences, then compute the similarity scores for each pair of sentence embeddings.\nFor each sentence aiin the answer, calculate the maximum similarity by\nSmax(ai)=max\n1≤j≤nSim(ai, cj),\nwhich measures how well the answer sentence aiis grounded in the context C={c1, c2, . . . , c n}.\nTo obtain an overall groundedness score for the entire answer, we may aggregate the maxi-\nmum similarity scores for all answer sentences:\nSgroundedness =1\nkk\n∑\ni=1Smax(ai).\nA high Sgroundedness score indicates that on average the sentences in the answer are well-\nsupported by the context, suggesting that the answer is grounded and less likely to contain\nhallucinations.\nConversely, a low Sgroundedness score suggests that some sentences in the answer may lack\nsufficient support from the context. The sentence with the lowest similarity to any context\nsentence, identified as\ni∗=arg min\n1≤i≤kSmax(ai),\nis considered the least grounded and may indicate a potential hallucination. Other sentences\nwith low Smax(ai)values could also signal possible hallucinations.\n3.2.2 Natural Language Inference (NLI)\nNLI models are specifically designed to determine the inferential relationship between two\npieces of text, a premise and a hypothesis, by classifying the relationship as “entailment”,\n“neutral”, or “contradiction” (MacCartney, 2009). In the context of RAG systems, we treat\nthe context as the premise and the generated answer as the hypothesis.\nNLI aims to determine whether a hypothesis can logically be inferred from a premise.\nFor the purpose of measuring groundedness (the opposite of hallucination), NLI provides a\nmechanism to assess whether the generated answer is logically supported by the context. If\nthe answer is entailed by the context, it is considered grounded; if it contradicts the context\nor is unrelated, it may indicate a hallucination.\nWhile NLI models provide class probabilities through multi-class classification, we can\nobtain a more nuanced groundedness measure by analyzing the embeddings produced by\nthe model and measuring the distance to the decision boundary. The decision boundary\nin the embedding space separates different classes and reflects the model’s confidence in its\npredictions. In this method, the distance to the decision boundary is directly related to the\n13\nlogit value for the “entailment” class. This approach simplifies the calculation and provides\nan interpretable measure of groundedness.\nSuppose a linear classifier computes a logit score zusing the embedding xof the input\n(which combines the premise and hypothesis):\nz=w⊺x+b\nwhere wis the weight vector and bis the bias term. The distance Dfrom the input point x\nto the decision boundary (i.e., the hyperplane w⊺x+b=0) is given by:\nD=w⊺x+b\n∥w∥=z\n∥w∥\nwhere∥w∥is the Euclidean norm (magnitude) of the weight vector. The distance to the\ndecision boundary can be used to measure the groundedness in the sense that\n•when D>0, the hypothesis is on the entailment side, i.e., grounded;\n•when D<0, the hypothesis is on the non-entailment side, i.e., potential hallucination.\nWe can map the distance Dto a probability groundedness score between 0 and 1 by applying\nthe logit transformation σ(D)=1/(1+e−D).\nIn sentence-level groundedness assessment, each sentence aiin the answer Ais combined\nwith the context C, then input into the NLI model to obtain the embedding xi. This allows\nus to compute the sentence-level zi, Diandσ(Di). The sentences with low σ(Di)scores may\nbe identified as potential hallucinations.\n3.3 Completeness\nCompleteness evaluates whether the generated answer covers all relevant information from\nthe context. In RAG systems, completeness refers to the degree to which the generated\nanswer incorporates all relevant information from the retrieved context. A complete answer\nshould not only be accurate and relevant but also cover all essential points that are pertinent\nto the user’s query. Ensuring completeness is crucial for providing users with comprehensive\nand informative responses.\n3.3.1 Sentence Similarity\nThis approach assesses completeness by evaluating how well the sentences in the context are\nreflected in the generated answer. Similar to context relevancy and groundedness, we break\ndown the context and answer into sentences, then calculate embedding-based similarity in\n14\nthe sentence level. For each sentence ciin the context C={c1, c2, . . . , c n}, calculate the\nmaximum similarity with any sentence in the answer A={a1, a2, . . . , a k}:\nSmax(ci)=max\n1≤j≤kSim(ci, aj).\nThis score indicates how well the context sentence ciis covered in the answer.\nSimilarly, we can aggregate Smax(ci)scores to obtain the overall completeness score by\neither the simple average\nScompleteness =1\nnn\n∑\ni=1Smax(ci),\nor the weighted average\nScompleteness =n\n∑\ni=1wi⋅Smax(ci),\nwhere wiis the weight assigned to the context sentence cisubject to wi≥0 and∑n\ni=1wi=1.\nA high completeness score indicates that the answer covers most of the content from the\ncontext, while a low completeness score suggests that significant portions of the context are\nnot reflected in the answer.\n3.3.2 Distribution Alignment Using Wasserstein Distance\nWhen assessing the completeness of an answer generated by LLMs, it is essential to measure\nhow well the answer captures the entire information distribution of the original context. The\nsentence similarity approach focuses on finding close matches between individual sentences,\nwhich may not fully reflect the answer’s coverage of the overall context. By applying Wasser-\nstein distance , a measure from optimal transport theory, we can evaluate the alignment of\ninformation distribution between the context and the summary, offering a complementary\nperspective on completeness; see also Tang et al. (2022).\nOptimal transport (Chewi et al., 2024) is a mathematical approach for measuring the\ncost of transforming one distribution into another. Wasserstein distance, also known as\nEarth Mover’s distance, is an optimal transport metric that quantifies the minimum cost\nto align two distributions, reflecting how closely they match in structure and content. In\nthe context of evaluating RAG-generated answers, the context sentences are treated as a\ndistribution of information that needs to be represented in the answer sentences. In this case,\nWasserstein distance measures how much effort is required to transform the distribution of\ncontext information Cinto the distribution of answer information A:\nW(C,A)=min\nγ∈Γ(p,q)n\n∑\ni=1k\n∑\nj=1γijd(ci, aj)\n15\nwhere d(ci, sj)is the distance (e.g., Euclidean or cosine distance) between the embeddings\nof context sentence ciand answer sentence aj,γijis the transport weight that represents\nhow much of context sentence ciis mapped to answer sentence aj, and Γ (p, q)is the set of\nall possible transport plans.\nThe goal is to find the transport plan γthat minimizes the total cost, yielding the\noptimal Wasserstein distance between the distributions. For simplicity, assume each sentence\ncontributes equally to the overall information distribution by assigning each sentence a weight\nof 1/nfor context sentences and 1 /kfor answer sentences. Then, calculate the average\ntransport cost, weighted by 1 /nkbased on the uniform weights. This involves summing the\npairwise distances and dividing by the total number of pairs,\nW(C,A)=1\nnkn\n∑\ni=1k\n∑\nj=1d(ci, aj).\nThis average distance provides a straightforward approximation of the Wasserstein distance.\nTo interpret the Wasserstein distance for completeness, a lower distance indicates that\nthe summary effectively captures the distribution of information in the context, suggesting\nhigher completeness. A higher Wasserstein distance implies gaps in completeness, where the\nanswer may be missing critical content from the context.\n3.3.3 Complementing Sentence Similarity with Wasserstein Distance\nWhile sentence similarity methods directly compare individual pairs of sentences, Wasser-\nstein distance provides a complementary approach by assessing the global alignment of in-\nformation across the entire context and answer. The key advantages of Wasserstein distance\nare:\n1.Global Information Distribution : Wasserstein distance captures the overall distribution of\ninformation in the context and answer, making it effective for assessing completeness\nby reflecting how well the summary represents the main ideas and topics from the\ncontext.\n2.Tolerance to Partial Matches : Unlike sentence similarity, which requires exact or near-\nexact matches, Wasserstein distance allows for approximate alignment. This means\nthat summaries with paraphrased or generalized content can still achieve low Wasser-\nstein distances, provided they retain the main themes.\n3.Contextual Relationships : By aligning entire sentence distributions, Wasserstein distance\nindirectly accounts for the thematic structure of the context and summary, offering a\nbroader perspective than sentence-level similarity alone.\n16\n4.Evaluating Completeness : A lower Wasserstein distance indicates that the answer closely\ncaptures the distribution of information from the context, reflecting higher complete-\nness. Conversely, a higher distance suggests missing content or inadequate coverage of\nessential topics.\nUsing both sentence similarity and Wasserstein distance together allows for a more nu-\nanced evaluation of completeness, capturing both the detailed alignment of specific sentences\nand the overall distribution of ideas and topics within the answer.\n3.4 Answer Relevancy\nAnswer relevancy ensures that the generated response directly addresses the user’s query.\nWe calculate maximum similarity scores between the query and the generated response to\nmeasure alignment with the user’s intent. Again, we can employ the sentence-level similarity\napproach similar to previously discussed metrics. This time we break down both the query\nand the answer into individual sentences and computing similarity scores for each pair. By\nanalyzing these similarities, we can quantify how well the answer addresses the user’s query.\nFor each sentence aiin the answer A={a1, a2, . . . , a k}, calculate the maximum similarity\nwith any sentence in the query Q={q1, q2, . . . , q m}:\nSmax(ai)=max\n1≤j≤mSim(ai, qj),\nwhich measures how well the answer sentence aiis relevant to the query. Then, to obtain an\noverall answer relevancy score for the entire answer, we can aggregate the Smax(ai)scores\nfor all answer sentences by either the simple average\nSa-relevancy=1\nkk\n∑\ni=1Smax(ai)\nor the weighted average\nSa-relevancy=k\n∑\ni=1wi⋅Smax(ai),\nwhere wiis the weight assigned to the answer sentence aisubject to wi≥0 and∑k\ni=1wi=1.\nA high Sa-relevancy score indicates that on average the answer effectively addresses the user’s\nquestion, while a low Sa-relevancy score indicates potential divergence or irrelevance.\nWhen it is critical that all parts of the answer are desired to be relevant to the query,\nwe may focus on the least relevant answer sentence and compute the minimum of maximum\nsimilarities:\nSa-relevancy=min\n1≤i≤kSmax(ai).\n17\n4 Risk and Safety Metrics\nIn this section, we provide a concise overview of evaluation metrics for assessing risk and\nsafety aspects of generative language models. Here we focus on critical dimensions that\nensure the deployed GLMs perform responsibly, and in alignment with regulatory standards\nfor appropriate use in high-stakes applications such as banking. Three essential risk and\nsafety metrics for validating GLMs in these environments include toxicity assessment, bias\nevaluation, and privacy protection.\nToxicity assessment measures the likelihood that a model generates harmful, offensive,\nor inappropriate content. In banking, where interactions must be professional and respect-\nful, toxicity in model outputs can severely damage customer trust, harm the institution’s\nreputation, and potentially lead to legal repercussions if sensitive or controversial topics\nare mishandled. Toxicity is typically assessed through NLI models that classify statements\nas safe or offensive, allowing for real-time toxicity assessment; see Hanu and Unitary team\n(2020) among others.\nBias evaluation focuses on detecting demographic or sentiment bias within the model re-\nsponses, ensuring equitable treatment of diverse user groups. In banking, where interactions\nmay influence financial decisions or customer perceptions, bias can lead to discriminatory\nresponses, harming the reputation of the institution and potentially leading to regulatory\nscrutiny. Bias evaluation involves testing the model with a diverse set of demographic-related\nqueries, including variations in race, gender, age, and income level, to determine if response\nquality or sentiment varies across different groups. Sentiment analysis models help identify\npotential differences in tone or attitude, while counterfactual evaluation assesses whether al-\ntering demographic-related terms (e.g., swapping “man” with “woman”) results in consistent\nresponses. Models are scored on bias based on thresholds that align with banking standards,\nallowing institutions to identify and mitigate unacceptable biases before deployment. If bi-\nases are identified, they can be addressed by fine-tuning the model with additional data\nthat represents underrepresented groups fairly, or by implementing constraints to reduce\nunintended biases in generated content.\nPrivacy protection is essential to ensure the model does not disclose sensitive infor-\nmation, such as personal financial details, customer identities, or other proprietary data. In\nbanking, privacy is paramount due to stringent regulatory requirements like the GDPR and\nCCPA, and privacy violations can result in severe financial and legal consequences. Privacy\nprotection for GLMs often involves Named Entity Recognition (NER), which identifies and\nflags sensitive entities in the output, such as names, addresses, or account numbers, allowing\nthe model to suppress or filter such information before it reaches the user. Additionally,\n18\ncontextual data filtering is implemented to identify phrases related to account transactions\nor other sensitive areas, reducing the risk of unintentional data leakage. Adversarial testing\nis used to simulate scenarios where users might try to elicit sensitive information, and the\nmodel responses are evaluated to ensure that they avoid privacy violations. Strict thresholds\nare set to flag any instance of sensitive information being revealed, prompting immediate\naction to investigate and adjust the model if necessary. Privacy safeguards are reinforced\nthrough model retraining or the implementation of guardrails, ensuring compliance with\nindustry regulations and protecting customer information.\nTogether, these metrics form a robust framework for risk and safety validation of GLMs,\nparticularly in banking. Each metric provides a unique lens for assessing the model behav-\nior, guiding institutions in minimizing risks associated with inappropriate content, unfair\ntreatment, and data privacy. This validation process helps ensure that models meet the\nhigh standards of accuracy, transparency, and accountability required in financial services,\nsupporting safe and responsible deployment in real-world applications.\n5 Calibration of Machine and Human Evaluations\nTo ensure alignment between machine-generated scores and human judgments, we employ a\ndouble-calibration approach. This process consists of two stages: probability calibration and\nconformal prediction , each playing a unique role in producing reliable machine evaluations.\nSince conformal prediction adds an additional layer of calibration, we refer to this two-stage\nprocess as the double-calibration method. Figure 3 provides a diagram of this two-stage\napproach, with further details described below:\n•Stage 1: Probability calibration provides an initial mapping of machine scores to prob-\nabilities that align with human expectations,\n•Stage 2: Conformal prediction quantifies the uncertainty of these calibrated probabil-\nities, providing prediction intervals with confidence levels.\nThis double-calibration strategy allows us to link machine evaluation metrics (such as\nrelevancy, groundedness, or completeness) to human evaluations that may be binary or multi-\nlevel. Error analysis (Type I and II errors) can be applied to assess the alignment of machine\nversus human evaluations. By setting calibrated thresholds on calibration model outputs,\nwe derive prediction sets that reflect the evaluation confidence, facilitating both automated\ndecision-making and human-in-the-loop processes.\n19\nMachine-Generated Scores\nHuman JudgmentsStage 1: Probability Calibration\nStage 2: Conformal Prediction\nFigure 3: Calibration Diagram of Machine and Human Evaluations\n5.1 Stage 1: Probability Calibration\nThe first stage of probability calibration aims to map raw machine-generated scores to cal-\nibrated probabilities that align with human judgments. This step translates machine eval-\nuations into a probability scale, making it easier to interpret machine-generated scores in\nterms of human expectations.\nTo achieve this, we employ standard probability calibration methods, which include:\n•Logistic Model : For human evaluations with binary labels, a logistic regression model is\nused to map machine scores to probabilities, producing a sigmoid-shaped probability\ncurve. This approach is also known as Platt scaling, and it is effective when machine\nscores have a roughly linear relationship with human judgments. For multi-category\nevaluations, it could involve probabilities assigned to each category.\n•Ordinal Regression : For human evaluations with rankings, ordinal regression provides a\ncalibrated probability for each level, reflecting the likelihood of each judgment category.\n•Isotonic Regression or Monotonic XGBoost : These non-parametric methods provide a\nflexible, piecewise-constant mapping of scores to probabilities, especially useful when\nthere is a monotonic relationship between machine scores and human evaluations.\nUsing a hold-out calibration dataset with both machine scores and human labels, we train\nthe chosen calibration model to learn the mapping from raw scores to probabilities. Once\ntrained, the model can be applied to any new machine-generated score, providing a calibrated\nprobability that represents the likelihood of human agreement.\n20\n5.2 Stage 2: Conformal Prediction\nConformal prediction is a flexible statistical framework that produces prediction intervals\nor sets for any model, assuming only data exchangeability (Vovk et al., 2005; Angelopoulos\nand Bates, 2023). It enables the creation of confidence sets that include the true label with\na specified confidence level, making it well-suited to classification tasks with complex or\nuncertain mappings between machine evaluation metrics and human labels. In this paper,\nwe consider only the split conformal prediction, while leaving the full conformal prediction\nto a future paper.\nIn our context, conformal prediction applies a second level of calibration to the calibrated\nprobabilities obtained from Stage 1. To implement this, we use an independent hold-out\ncalibration sample, distinct from the sample used for probability calibration. This separate\ndataset includes machine-generated scores and corresponding human labels, allowing us to\nquantify the uncertainty around the calibrated probabilities established in Stage 1.\nWe discuss the split conformal prediction for the logistic calibration model ˆf(⋅). In the\nprocedure below, S(x, y, ˆf)denotes the non-conformity score, which measures the deviation\nbetween the calibrated probability and the human evaluation. This notation is distinct from\nother uses of Sin this paper.\n1.Non-Conformity Score Calculation : For each sample in the independent calibration set\nwith sample size n, compute the non-conformity score\nS(x, y, ˆf)=∣y−ˆf(x)∣=1−P(Y=y∣x),\nwhere ˆf(x)is the calibrated probability from Stage 1, and yis the observed human\nlabel 1 or 0.\n2.Calibrated Quantile Computation : Choose a desired confidence level, represented by an\nerror rate α(e.g., α=0.1 for 90% confidence). Then compute the quantile ˆ qof the\nnon-conformity scores as:\nˆq=Quantile ({S1, S2, . . . , S n};⌈(n+1)(1−α)⌉\nn+1).\n3.Prediction Set Construction : For a new test sample xtest, create the prediction set\nT(xtest)by including all possible values ysuch that:\nT(xtest)={y∶S(xtest, y,ˆf(xtest))≤ˆq}={y∶P(Y=y∣xtest)≥1−ˆq}.\nFor each new instance xtest, the prediction set is among the three possible cases depending\non the predicted probability ˆf(xtest)and the calibrated quantile ˆ q.\n21\n•Single-Class Set :{0}when ˆf(xtest)<min{ˆq,1−ˆq}, or{1}when ˆf(xtest)>max{ˆq,1−ˆq}.\n•Both-Class Set :{0, 1}when 1−ˆq≤ˆf(xtest)≤ˆqand ˆq≥0.5.\n•Empty Set :∅when ˆ q<ˆf(xtest)<1−ˆqand ˆq<0.5.\nThe single-class prediction set indicates high confidence that the true label corresponds to\nthis class. For instance, a prediction set at 90% confidence level suggests a high likelihood\nthat the true evaluation aligns with this class. On the other hand, when the prediction set\nincludes both classes or empty, it reflects uncertainty and suggests that additional review\nmay be needed. Figure 4 presents an example of machine-human calibration using logistic\nregression for groundedness evaluation with binary labels from human judgment.\nFigure 4: An illustration of calibration for machine-human groundedness evaluation, using\nlogistic regression and conformal prediction.\n6 Robustness and Weakness Analysis\nThis section provides an overview of two essential perspectives in the testing and validation\nof GLMs: robustness testing and weakness identification. Robustness testing examines the\nstability and resilience of the model under varied input conditions, while weakness identifi-\ncation aims to pinpoint specific areas where the model may need improvement. Together,\nthese evaluations offer insights into the model’s strengths and potential vulnerabilities, guid-\ning enhancements before deployment in critical applications.\n22\n6.1 Robustness Testing\nRobustness testing assesses the ability of a RAG system to handle diverse and challenging\ninputs (including potentially problematic inputs), ensuring that it performs robustly across\na range of real-world scenarios. The testing includes three main types of robustness checks:\n1.Adversarial Inputs : This involves introducing deliberately misleading or contradictory\ninformation to the model’s input. Adversarial testing can expose how the model han-\ndles conflicting information and whether it can distinguish relevant content from dis-\ntractors. By subjecting the model to adversarial queries, evaluators can assess its\nability to avoid generating inaccurate or biased responses, which is particularly impor-\ntant in regulated industries like banking where factual accuracy is critical.\n2.Out-of-Distribution Queries : To examine the model’s adaptability, robustness testing\nincludes queries on topics that are not present within the model’s training data or\ndocument collection. Out-of-distribution queries help reveal the model’s limitations\nby testing how it responds to unfamiliar topics. A robust RAG model should either\nrespond appropriately based on the closest relevant information or acknowledge its\nlimitations instead of generating inaccurate information.\n3.Input Variations : The system is also evaluated on its ability to handle input variations,\nincluding spelling errors, grammatical mistakes, and colloquial language. This type of\ntesting is essential for real-world applications where user inputs may be unstructured\nor contain errors. A resilient RAG system should be able to interpret such variations\nand generate coherent responses despite minor input inaccuracies.\nBy systematically examining the model performance across these query types, the ro-\nbustness testing phase highlights potential weaknesses, guiding targeted improvements. This\nensures that the model remains reliable and resilient across various practical scenarios and\ninput complexities, enhancing its robustness for deployment in critical applications.\n6.2 Weakness Identification\nModel weakness identification, provides an approach for pinpointing specific performance is-\nsues within a RAG system. This process allows for a granular understanding of the model’s\nareas of under performance, supporting targeted improvements. The methodology in weak-\nness identification includes the following techniques:\n1.Marginal Analysis : It is a key technique where the system’s performance is evaluated\nacross individual dimensions, such as topics or query types. By breaking down metrics\n23\nfor each category, marginal analysis helps identify specific areas where the model does\nnot perform well. For instance, by analyzing relevance or groundedness scores for each\ntopic individually, the evaluation can reveal topics that require further data augmenta-\ntion or model fine-tuning. Marginal analysis is critical for finding isolated weaknesses\nthat might be masked in aggregate evaluations. Figures 5–7 show examples of marginal\nplots of various metrics. Each point is the evaluation result from each query, low values\nare where the weakness are.\n2.Bivariate Analysis : This method examines the interaction between two dimensions,\nsuch as topic and query type, to uncover joint weaknesses. This approach is beneficial\nfor identifying compound issues that may not be apparent when looking at a single\ndimension alone. For example, a model may perform adequately on simple questions\nwithin a topic but struggle with more complex, multi-hop questions in the same area.\nThis joint examination helps in pinpointing specific combinations that are challenging\nfor the model and might benefit from additional training or refinement strategies.\n3.Visualization Techniques : Tools such as heatmap and violin plots represent performance\ndistributions across different metrics, making it easier to communicate and interpret\nweaknesses. For example, heatmaps can visually indicate low-performance areas in\nterms of recall, precision, or relevance, allowing stakeholders to identify specific prob-\nlem areas quickly. Visualization supports the practical implementation of weakness\nidentification by clearly showing areas where performance dips, guiding model im-\nprovement initiatives in an accessible format.\nTogether, these methods provide a strategy for detailed, data-driven analysis of model\nweaknesses, guiding focused enhancement efforts to optimize the RAG system’s effectiveness\nand reliability before deployment.\n7 Discussion and Conclusion\nThis paper has presented a HCAT framework for testing and validation of GLMs, specifically\ntailored for RAG systems used in high-stakes applications like banking. By leveraging the\nstructured, bounded nature of RAG systems, where generation is constrained by a defined\ndocument collection, we address the inherent challenges of evaluating GLMs in open-ended\ndomains. Our proposed framework provides a robust, scalable, and transparent solution for\ntesting GLMs, integrating multiple validation methodologies to enhance model reliability,\ninterpretability, and compliance.\n24\nFigure 5: Marginal (topic) weakness analysis: recall & precision\nFigure 6: Marginal (topic) weakness analysis: relevancy (sentence recall & precision)\nFigure 7: Marginal (topic) weakness analysis: groundedness & answer relevancy\n25\nThe framework presented here offers several key benefits:\n•Comprehensive Testing : Automatic query generation through stratified sampling based\non topic modeling ensures thorough coverage of the knowledge base. This systematic\napproach allows us to generate diverse and representative queries that test the model\nacross all relevant topics and query types.\n•Explainable Evaluation Metrics : By employing embedding-based evaluation metrics, in-\ncluding embeddings trained through contrastive learning and specialized embeddings\nfrom NLI models, we provide transparent and interpretable assessments. This avoids\nreliance on black-box tools and allows for in-depth analysis of semantic relevance,\ngroundedness, and logical consistency between queries, documents, and responses.\n•Trustworthy Evaluations : Calibration with human judgments aligns machine evalua-\ntions with human perceptions, enhancing the reliability of automated assessments. By\naccounting for the limitations of algorithmic metrics, we ensure that the evaluation\nreflects qualities valued by users.\n•Robustness Assessment : It evaluates the model stability against varied scenarios, such\nas adversarial inputs, out-of-distribution queries, and linguistic variations, ensuring re-\nliable performance in diverse practical contexts. This comprehensive robustness testing\nuncovers vulnerabilities, ensuring consistent performance under challenging conditions.\n•Targeted Improvements : Weakness identification through marginal and bivariate analy-\nsis enables focused enhancements. By pinpointing specific topics or query types where\nthe model underperforms, we can prioritize areas for improvement and optimize the\nsystem’s overall effectiveness.\nWhile the framework addresses many challenges, caution must be taken due to certain\nlimitations such as:\n•Quality of Topic Modeling : The effectiveness of stratified sampling depends on the\naccuracy of topic modeling. Inaccurate or overly broad topics may lead to insufficient\ncoverage or imbalanced sampling.\n•Human Calibration Sample Size : The representativeness of human evaluations may be\nlimited by sample size and diversity. Expanding the calibration dataset with more\nextensive and diverse human judgments can improve alignment between machine eval-\nuations and human perceptions, leading to more accurate calibration.\n26\n•Evolving Language Models : As generative language models continue to advance, evalu-\nation methods must adapt accordingly. Ongoing research into embedding techniques\nand evaluation metrics is necessary to keep pace with model developments and ensure\nthe framework remains effective and relevant.\nIn conclusion, the proposed framework addresses the critical need for comprehensive and\nreliable evaluation of generative language models, particularly within the context of RAG\nsystems. By systematically combining automatic test generation, explainable and human-\ncalibrated evaluation metrics, robustness testing, and targeted weakness identification, we\nenhance the trustworthiness of these models.\nReferences\nAngelopoulos, A. N. and Bates, S. (2023). Conformal prediction: A gentle introduction.\nFoundations and Trends in Machine Learning ,16(4), pp. 494–591.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., . . . , and Amodei,\nD. (2020). Language models are few-shot learners. Advances in Neural Information Pro-\ncessing Systems ,33, pp. 1877–1901.\nChewi, S., Niles-Weed, J. and Rigollet, P. (2024). Statistical optimal transport. arXiv\npreprint: 2407.18163\nDevlin, J., Chang, M. W., Lee, K. and Toutanova, K. (2019). BERT: Pre-training of deep\nbidirectional transformers for language understanding. Proceedings of the 2019 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers) , pp. 4171–4186.\nGao, T., Yao, X. and Chen, D. (2021), SimCSE: Simple contrastive learning of sentence em-\nbeddings. Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing , pp. 6894–6910.\nGrootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF\nprocedure. arXiv preprint: 2203.05794 .\nHanu, L. and Unitary team. (2020). Detoxify. https://github.com/unitaryai/detoxify\nJigsaw and Google. (2017). Perspective API. https://www.perspectiveapi.com/\n27\nKhashabi, D., Stanovsky, G., Bragg, J., Lourie, N., Kasai, J., Choi, Y.,. . . , and Weld, D.\n(2021). QAFactEval: Improved QA-based factual consistency evaluation. arXiv preprint:\n2112.08542 .\nKryscinski, W., McCann, B., Xiong, C. and Socher, R. (2020). Evaluating the factual con-\nsistency of abstractive text summarization. arXiv preprint: 2004.04228 .\nKrishna, K., Khattab, G., Moschella, R., Zhao, P. and Zhang, C. (2023). RAG: A compre-\nhensive survey of retrieval-augmented text generation. arXiv preprint: 2312.10997 .\nLaban, P., Hirst, L., Cummings, J. and Bansal, M. (2022). SummaC: Re-visiting NLI-based\nmodels for inconsistency detection in summarization. Transactions of the Association for\nComputational Linguistics , 10, pp. 163–177.\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., . . . , and Kiela,\nD. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in\nNeural Information Processing Systems ,33, pp. 9459–9474.\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., . . . , and Koreeda,\nY. (2023). Holistic evaluation of language models. Transactions on Machine Learning\nResearch .\nLin, S., Hilton, J. and Evans, O. (2022). TruthfulQA: Measuring how models mimic human\nfalsehoods. Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics , Volume 1: Long Papers (pp. 3214–3252).\nLi, Y., Singh, R., Joshi, T. and Sudjianto,A. (2024). Automatic generation of behavioral\ntest cases for natural language processing using clustering and prompting. arXiv preprint:\n2408.00161 .\nMacCartney, B. (2009). Natural Language Inference . Doctoral Dissertation, Stanford Uni-\nversity.\nMcInnes, L., Healy, J. and Melville, J. (2018). UMAP: Uniform manifold approximation and\nprojection for dimension reduction. arXiv preprint: 1802.03426 .\nReimers, N. and Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese\nBERT-networks. Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , pp. 3982—3992.\n28\nRibeiro, M. T., Wu, T., Guestrin, C. and Singh, S. (2020). Beyond accuracy: behavioral\nTesting of NLP models with checkList. Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pp. 4902–4912.\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., and Xu, X. (2017). DBSCAN revisited,\nrevisited: Why and how you should (still) use DBSCAN. ACM Transactions on Database\nSystems , 42(3), pp. 1–21.\nSudjianto, A. and Zhang, A. (2024). Model validation practice in banking: A structured\napproach for predictive models. Available at SSRN: https://ssrn.com/abstract=4977043 .\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A., Abid, A., Fisch, A., . . . , and Mehta,\nH. (2023). Beyond the imitation game: Quantifying and extrapolating the capabilities of\nlanguage models. Transactions on Machine Learning Research .\nTang, P., Hu, K., Yan, R., Zhang, L., Gao, J. and Wang, Z. (2022). OTExtSum: Extractive\ntext summarisation with optimal transport. Findings of the Association for Computational\nLinguistics: NAACL 2022 , pp. 1128–1141.\nVovk, V., Gammerman, A. and Shafer, G. (2005). Algorithmic learning in a random world.\nSpringer Science & Business Media .\nYang, Z. Qi, P., Zhang, S., Bengio, Y., Cohen, W.W., Salakhutdinov, R. and Manning, C.D.\n(2018). HotpotQA: A dataset for diverse, explainable multi-hop question answering. arXiv\nPreprint: 1809.09600 .\nZhang, T., Kishore, V., Wu, F., Weinberger, K. Q. and Artzi, Y. (2020). BERTScore: Evalu-\nating text generation with BERT. International Conference on Learning Representations .\nZhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., . . . , and Wen, J. R. (2023). A\nsurvey of large language models. arXiv preprint: 2303.18223 .\n29",
    "content_summary": "Human-Calibrated Automated Testing and Validation\nof Generative Language Models: An Overview\nAgus Sudjianto1,2, Aijun Zhang3,∗, Srinivas Neppalli1,\nTarun Joshi3,∗, Michal Malohlava1\nAbstract\nThis paper introduces a comprehensive framework for the eva...",
    "content_length": 62082,
    "created_at": "2025-07-18T16:41:04.861257+00:00",
    "updated_at": "2025-07-18T16:41:18.893694+00:00",
    "file_path": "2411.16391v2.pdf",
    "chunks_list": []
  }
}